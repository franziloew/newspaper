
@article{_germany_,
  title = {Germany’s Far-Right Party Will Make the {{Bundestag}} Much Noisier},
  journaltitle = {The Economist},
  issn = {0013-0613},
  url = {https://www.economist.com/europe/2017/08/24/germanys-far-right-party-will-make-the-bundestag-much-noisier},
  urldate = {2022-06-12},
  entrysubtype = {magazine},
  file = {/Users/franzilow/Zotero/storage/EXSYQZZC/germanys-far-right-party-will-make-the-bundestag-much-noisier.html}
}

@online{_media_2014,
  title = {Media {{Freedom}} and {{Pluralism}}},
  date = {2014-02-17},
  url = {https://ec.europa.eu/digital-single-market/en/policies/media-freedom-and-pluralism},
  urldate = {2018-03-14},
  abstract = {The importance of transparency, freedom and diversity in Europe's media landscape. The European Commission commits to respect freedom and pluralism of media. In this page you can find several acts, documents and studies on the subject.},
  langid = {english},
  organization = {{Digital Single Market}},
  file = {/Users/franzilow/Zotero/storage/23P77YVX/media-freedom-and-pluralism.html}
}

@book{airoldi_handbook_2014,
  title = {Handbook of Mixed Membership Models and Their Applications},
  author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Blei, David},
  date = {2014},
  publisher = {{Taylor and Francis}},
  url = {http://cds.cern.ch/record/1974849},
  urldate = {2017-10-12},
  abstract = {In response to scientific needs for more diverse and structured explanations of statistical data, researchers have discovered how to model individual data points as belonging to multiple groups. Handbook of Mixed Membership Models and Their Applications shows you how to use these flexible modeling tools to uncover hidden patterns in modern high-dimensional multivariate data. It explores the use of the models in various application settings, including survey data, population genetics, text analysis, image processing and annotation, and molecular biology.Through examples using real data sets, yo},
  isbn = {978-1-4665-0408-0},
  file = {/Users/franzilow/Zotero/storage/P7XE3MF4/1974849.html}
}

@article{airoldi_improving_2016,
  title = {Improving and {{Evaluating Topic Models}} and {{Other Models}} of {{Text}}},
  author = {Airoldi, Edoardo M. and Bischof, Jonathan M.},
  date = {2016-10-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1381--1403},
  issn = {0162-1459},
  doi = {10.1080/01621459.2015.1051182},
  url = {http://dx.doi.org/10.1080/01621459.2015.1051182},
  abstract = {An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parameterizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. Here, we show that words that are both frequent and exclusive to a theme are more effective at characterizing topical content, and we propose a regularization scheme that leads to better estimates of these quantities. We consider a supervised setting where professional editors have annotated documents to topic categories, organized into a tree, in which leaf-nodes correspond to more specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze these annotated documents. A parallelized Hamiltonian Monte Carlo sampler allows the inference to scale to millions of documents. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. In this supervised setting, we validate the efficacy of word frequency and exclusivity at characterizing topical content on two very large collections of documents, from Reuters and the New York Times. In an unsupervised setting, we then consider a simplified version of the model that shares the same regularization scheme with the previous model. We carry out a large randomized experiment on Amazon Mechanical Turk to demonstrate that topic summaries based on frequency and exclusivity, estimated using the proposed regularization scheme, are more interpretable than currently established frequency-based summaries, and that the proposed model produces more efficient estimates of exclusivity than the currently established models.},
  keywords = {Categorical data,Hamiltonian Monte Carlo,High-dimensional data,Parallel inference,text analysis},
  file = {/Users/franzilow/Zotero/storage/HWNC8RRH/01621459.2015.html}
}

@article{airoldi_reconceptualizing_2010,
  title = {Reconceptualizing the Classification of {{PNAS}} Articles},
  author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Joutard, Cyrille and Love, Tanzy and Shringarpure, Suyash},
  date = {2010-12-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {107},
  number = {49},
  eprint = {21078953},
  eprinttype = {pmid},
  pages = {20899--20904},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1013452107},
  url = {http://www.pnas.org/content/107/49/20899},
  urldate = {2017-10-07},
  abstract = {PNAS article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying PNAS research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in PNAS.},
  langid = {english},
  keywords = {Dirichlet process,hierarchical modeling,Monte Carlo Markov chain,text analysis,variational inference},
  file = {/Users/franzilow/Zotero/storage/3HPSSBQ2/Airoldi et al. - 2010 - Reconceptualizing the classification of PNAS artic.pdf;/Users/franzilow/Zotero/storage/3SR8QVPZ/20899.html}
}

@inproceedings{alsumait_topic_2009,
  title = {Topic {{Significance Ranking}} of {{LDA Generative Models}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {AlSumait, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta},
  date = {2009-09-07},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {67--82},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-04180-8_22},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-04180-8_22},
  urldate = {2017-11-16},
  abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
  eventtitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  isbn = {978-3-642-04179-2 978-3-642-04180-8},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/WCPH8XR4/10.html}
}

@incollection{anderson_media_2006,
  title = {The {{Media}} and {{Advertising}}: {{A Tale}} of {{Two-Sided Markets}}},
  shorttitle = {Chapter 18 {{The Media}} and {{Advertising}}},
  booktitle = {Handbook of the {{Economics}} of {{Art}} and {{Culture}}},
  author = {Anderson, Simon P. and Gabszewicz, Jean J.},
  editor = {Ginsburg, Victor A. and Throsby, David},
  date = {2006-01-01},
  volume = {1},
  pages = {567--614},
  publisher = {{Elsevier}},
  doi = {10.1016/S1574-0676(06)01018-0},
  url = {http://www.sciencedirect.com/science/article/pii/S1574067606010180},
  urldate = {2019-04-19},
  abstract = {Media industries are important drivers of popular culture. A large fraction of leisure time is devoted to radio, magazines, newspapers, the Internet, and television (the illustrative example henceforth). Most advertising expenditures are incurred for these media. They are also mainly supported by advertising revenue. Early work stressed possible market failures in program duplication and catering to the Lowest Common Denominator, indicating lack of cultural diversity and quality. The business model for most media industries is underscored by advertisers' demand to reach prospective customers. This business model has important implications for performance in the market since viewer sovereignty is indirect. Viewers are attracted by programming, though they dislike the ads it carries, and advertisers want viewers as potential consumers. The two sides are coordinated by broadcasters (or “platforms”) that choose ad levels and program types, and advertising finances the programming. Competition for viewers of the demographics most desired by advertisers implies that programming choices will be biased towards the tastes of those with such demographics. The ability to use subscription pricing may help improve performance by catering to the tastes of those otherwise under-represented, though higher full prices tend to favor broadcasters at the expense of viewers and advertisers. If advertising demand is weak, program equilibrium program selection may be too extreme as broadcasters strive to avoid ruinous subscription price competition, but strong advertising demand may lead to strong competition for viewers and hence minimum differentiation (“la pensée unique”). Markets (such as newspapers) with a high proportion of ad-lovers may be served only by monopoly due to a circulation spiral: advertisers want to place ads in the paper with most readers, but readers want to buy the paper with more ads.},
  keywords = {advertising finance,circulation spiral,pensee unique,platform competition,two-sided markets},
  file = {/Users/franzilow/Zotero/storage/SVCVBE4B/S1574067606010180.html}
}

@article{angrist_using_1999,
  title = {Using {{Maimonides}}' {{Rule}} to {{Estimate}} the {{Effect}} of {{Class Size}} on {{Scholastic Achievement}}},
  author = {Angrist, Joshua D. and Lavy, Victor},
  date = {1999},
  journaltitle = {The Quarterly Journal of Economics},
  volume = {114},
  number = {2},
  pages = {533--575},
  publisher = {{Oxford University Press}},
  issn = {0033-5533},
  url = {https://www.jstor.org/stable/2587016},
  urldate = {2021-06-20},
  abstract = {The twelfth century rabbinic scholar Maimonides proposed a maximum class size of 40. This same maximum induces a nonlinear and nonmonotonic relationship between grade enrollment and class size in Israeli public schools today. Maimonides' rule of 40 is used here to construct instrumental variables estimates of effects of class size on test scores. The resulting identification strategy can be viewed as an application of Donald Campbell's regression-discontinuity design to the class-size question. The estimates show that reducing class size induces a significant and substantial increase in test scores for fourth and fifth graders, although not for third graders.}
}

@article{armstrong_competition_2006,
  title = {Competition in Two-Sided Markets},
  author = {Armstrong, Mark},
  date = {2006},
  journaltitle = {The RAND Journal of Economics},
  volume = {37},
  number = {3},
  pages = {668--691},
  issn = {1756-2171},
  doi = {10.1111/j.1756-2171.2006.tb00037.x},
  url = {http://dx.doi.org/10.1111/j.1756-2171.2006.tb00037.x}
}

@article{arora_practical_2012,
  title = {A {{Practical Algorithm}} for {{Topic Modeling}} with {{Provable Guarantees}}},
  author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  date = {2012-12-19},
  url = {https://arxiv.org/abs/1212.4777},
  urldate = {2017-12-07},
  file = {/Users/franzilow/Zotero/storage/KR8KRPIX/Arora et al. - 2012 - A Practical Algorithm for Topic Modeling with Prov.pdf;/Users/franzilow/Zotero/storage/P63RKV2V/1212.html}
}

@unpublished{asuncion_smoothing_2012,
  title = {On {{Smoothing}} and {{Inference}} for {{Topic Models}}},
  author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye},
  date = {2012-05-09},
  eprint = {1205.2662},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1205.2662},
  abstract = {Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/franzilow/Zotero/storage/MRRAS29S/Asuncion et al. - 2012 - On Smoothing and Inference for Topic Models.pdf;/Users/franzilow/Zotero/storage/UHH4ICFA/1205.html}
}

@article{baker_measuring_2016,
  title = {Measuring {{Economic Policy Uncertainty}}},
  author = {Baker, Scott R. and Bloom, Nicholas and Davis, Steven J.},
  date = {2016},
  journaltitle = {Quarterly Journal of Economics},
  number = {4},
  pages = {1593--1636},
  doi = {10.3386/w21633},
  url = {http://www.nber.org/papers/w21633},
  abstract = {We develop a new index of economic policy uncertainty (EPU) based on newspaper coverage frequency. Several types of evidence – including human readings of 12,000 newspaper articles – indicate that our index proxies for movements in policy-related economic uncertainty. Our US index spikes near tight presidential elections, Gulf Wars I and II, the 9/11 attacks, the failure of Lehman Brothers, the 2011 debt-ceiling dispute and other major battles over fiscal policy. Using firm-level data, we find that policy uncertainty raises stock price volatility and reduces investment and employment in policy-sensitive sectors like defense, healthcare, and infrastructure construction. At the macro level, policy uncertainty innovations foreshadow declines in investment, output, and employment in the United States and, in a panel VAR setting, for 12 major economies. Extending our US index back to 1900, EPU rose dramatically in the 1930s (from late 1931) and has drifted upwards since the 1960s.}
}

@unpublished{balahur_sentiment_2013,
  title = {Sentiment {{Analysis}} in the {{News}}},
  author = {Balahur, Alexandra and Steinberger, Ralf and Kabadjov, Mijail and Zavarella, Vanni and family=Goot, given=Erik, prefix=van der, useprefix=true and Halkia, Matina and Pouliquen, Bruno and Belyaeva, Jenya},
  date = {2013-09-24},
  eprint = {1309.6202},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1309.6202},
  urldate = {2018-11-15},
  abstract = {Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles - author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which (a) we test the relative suitability of various sentiment dictionaries and (b) we attempt to separate positive or negative opinion from good or bad news. In the experiments described here, we tested whether or not subject domain-defining vocabulary should be ignored. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,H.3.1,H.3.3,I.2.7,J.4},
  file = {/Users/franzilow/Zotero/storage/AGKXBNSY/Balahur et al. - 2013 - Sentiment Analysis in the News.pdf;/Users/franzilow/Zotero/storage/UKL3LFKZ/1309.html}
}

@book{balasubramanyan_tweets_2010,
  title = {From {{Tweets}} to {{Polls}} : {{Linking Text Sentiment}} to {{Public Opinion Time Series}}},
  shorttitle = {From {{Tweets}} to {{Polls}}},
  author = {Balasubramanyan, Ramnath and Routledge, Bryan R. and Smith, Noah A.},
  date = {2010},
  abstract = {We connect measures of public opinion measured from polls with sentiment measured from text. We analyze several surveys on consumer confidence and political opinion over the 2008 to 2009 period, and find they correlate to sentiment word frequencies in contemporaneous Twitter messages. While our results vary across datasets, in several cases the correlations are as high as 80\%, and capture important large-scale trends. The results highlight the potential of text streams as a substitute and supplement for traditional polling.},
  file = {/Users/franzilow/Zotero/storage/5NZK966N/Balasubramanyan et al. - 2010 - From Tweets to Polls  Linking Text Sentiment to P.pdf;/Users/franzilow/Zotero/storage/27STA53M/summary.html}
}

@article{baron_persistent_2006,
  title = {Persistent Media Bias},
  author = {Baron, David P.},
  date = {2006-01-01},
  journaltitle = {Journal of Public Economics},
  shortjournal = {Journal of Public Economics},
  volume = {90},
  number = {1},
  pages = {1--36},
  issn = {0047-2727},
  doi = {10.1016/j.jpubeco.2004.10.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0047272705000216},
  urldate = {2019-01-19},
  abstract = {The news media plays an essential role in society, but surveys indicate that the public views the media as biased. This paper presents a theory of media bias that originates with private information obtained by journalists through their investigations and persists despite profit-maximizing news organizations and rivalry from other news organizations. Bias has two effects on the demand for news. First, rational individuals are more skeptical of potentially biased news and thus rely less on it in their decision-making. This skepticism reduces demand and leads the news organization to set a lower price for its publication the greater is the bias it tolerates. Lower quality news thus commands a lower price. Second, bias makes certain stories more likely than others. Given their private information, journalists may bias their stories if their career prospects can be advanced by being published on the front page. News organizations can control bias by restricting the discretion allowed to journalists, but granting discretion and tolerating bias can increase profits if it allows journalists to be hired at a lower wage. Bias is not driven from the market by a rival news organization nor by a news organization with an opposing bias, and the profits of a high-bias news organization can be higher than the profits of a low bias one. Moreover, bias can be greater with competition than with a monopoly news organization. If individuals collectively choose regulation in place of their individual decision-making, bias increases the expected stringency of regulation.},
  keywords = {Bias,Media,News organizations},
  file = {/Users/franzilow/Zotero/storage/ESVNXLRG/S0047272705000216.html}
}

@unpublished{baturo_what_2017,
  title = {What {{Drives}} the {{International Development Agenda}}? {{An NLP Analysis}} of the {{United Nations General Debate}} 1970-2016},
  shorttitle = {What {{Drives}} the {{International Development Agenda}}?},
  author = {Baturo, Alexander and Dasandi, Niheer and Mikhaylov, Slava J.},
  date = {2017-08-19},
  eprint = {1708.05873},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.05873},
  abstract = {There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/Users/franzilow/Zotero/storage/FSRGQVHA/Baturo et al. - 2017 - What Drives the International Development Agenda .pdf;/Users/franzilow/Zotero/storage/H2JRUWQF/1708.html}
}

@article{benewick_floating_1969,
  title = {{{THE FLOATING VOTER AND THE LIBERAL VIEW OF REPRESENTATIONa}}},
  author = {Benewick, R. J. and Birch, A. H. and Blumler, J. G. and Ewbank, Alison},
  date = {1969-06-01},
  journaltitle = {Political Studies},
  volume = {17},
  number = {2},
  pages = {177--195},
  issn = {1467-9248},
  doi = {10.1111/j.1467-9248.1969.tb00634.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9248.1969.tb00634.x},
  urldate = {2018-08-14},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/Z53NBN9F/Benewick et al. - 1969 - THE FLOATING VOTER AND THE LIBERAL VIEW OF REPRESE.pdf;/Users/franzilow/Zotero/storage/C96GQEHU/j.1467-9248.1969.tb00634.html}
}

@article{bennett_new_2008,
  title = {A {{New Era}} of {{Minimal Effects}}? {{The Changing Foundations}} of {{Political Communication}}},
  shorttitle = {A {{New Era}} of {{Minimal Effects}}?},
  author = {Bennett, W. Lance and Iyengar, Shanto},
  date = {2008-12-01},
  journaltitle = {Journal of Communication},
  volume = {58},
  number = {4},
  pages = {707--731},
  issn = {1460-2466},
  doi = {10.1111/j.1460-2466.2008.00410.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.2008.00410.x},
  urldate = {2018-10-20},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/G2DCD998/Bennett und Iyengar - 2008 - A New Era of Minimal Effects The Changing Foundat.pdf;/Users/franzilow/Zotero/storage/9D8T2CAU/j.1460-2466.2008.00410.html}
}

@article{besley_handcuffs_2006,
  title = {Handcuffs for the {{Grabbing Hand}}? {{Media Capture}} and {{Government Accountability}}},
  shorttitle = {Handcuffs for the {{Grabbing Hand}}?},
  author = {Besley, Timothy and Prat, Andrea},
  date = {2006-06},
  journaltitle = {American Economic Review},
  volume = {96},
  number = {3},
  pages = {720--736},
  issn = {0002-8282},
  doi = {10.1257/aer.96.3.720},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.96.3.720},
  urldate = {2019-03-30},
  abstract = {It has long been recognized that the media play an essential role in government accountability. Even in the absence of censorship, however, the government may influence news content by maintaining a "cozy" relationship with the media. This paper develops a model of democratic politics in which media capture is endogenous. The model offers insights into the features of the media market that determine the ability of the government to exercise such capture and hence to influence political outcomes. (JEL D72, D73, L82)},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/6H3PQSX8/Besley and Prat - 2006 - Handcuffs for the Grabbing Hand Media Capture and.pdf;/Users/franzilow/Zotero/storage/GPVE7D97/articles.html}
}

@article{bholat_text_2015,
  title = {Text {{Mining}} for {{Central Banks}}},
  author = {Bholat, David M. and Hansen, Stephen and Santos, Pedro M. and Schonhardt-Bailey, Cheryl},
  date = {2015-06-29},
  journaltitle = {SSRN Electronic Journal},
  issn = {1556-5068},
  url = {http://www.academia.edu/13430482/Text_mining_for_central_banks},
  urldate = {2017-11-06},
  abstract = {Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful},
  file = {/Users/franzilow/Zotero/storage/J462FFQU/Text_mining_for_central_banks.html}
}

@inproceedings{bischof_summarizing_2012,
  title = {Summarizing {{Topical Content}} with {{Word Frequency}} and {{Exclusivity}}},
  booktitle = {Proceedings of the 29th {{International Coference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
  date = {2012},
  series = {{{ICML}}'12},
  pages = {9--16},
  publisher = {{Omnipress}},
  location = {{USA}},
  url = {http://dl.acm.org/citation.cfm?id=3042573.3042578},
  urldate = {2018-01-19},
  abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
  isbn = {978-1-4503-1285-1}
}

@article{blair_pricing_1993,
  title = {Pricing {{Decisions}} of the {{Newspaper Monopolist}}},
  author = {Blair, Roger D. and Romano, Richard E.},
  date = {1993},
  journaltitle = {Southern Economic Journal},
  volume = {59},
  number = {4},
  pages = {721--732},
  issn = {0038-4038},
  doi = {10.2307/1059734},
  url = {http://www.jstor.org/stable/1059734}
}

@article{blassnig_hitting_2019,
  title = {Hitting a {{Nerve}}: {{Populist News Articles Lead}} to {{More Frequent}} and {{More Populist Reader Comments}}},
  shorttitle = {Hitting a {{Nerve}}},
  author = {Blassnig, Sina and Engesser, Sven and Ernst, Nicole and Esser, Frank},
  date = {2019-08-16},
  journaltitle = {Political Communication},
  shortjournal = {Political Communication},
  pages = {1--23},
  doi = {10.1080/10584609.2019.1637980},
  abstract = {Although research on effects of populist communication has increased, it is still unclear how populism in news articles affects the readers’ manifest behavior, such as whether and how they comment on online news. To address these issues, we conducted a content analysis of online news articles (N = 332) and corresponding reader comments (N = 2786) during election campaigns in France, Switzerland, and the United Kingdom. We find that populist key messages by political and media actors in news articles do not only provoke more reader comments but also prompt citizens to use populist key messages themselves in their comments – regardless of how journalists contextualize these statements.}
}

@inproceedings{blei_dynamic_2006,
  title = {Dynamic {{Topic Models}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Machine Learning}}},
  author = {Blei, David M. and Lafferty, John D.},
  date = {2006},
  series = {{{ICML}} '06},
  pages = {113--120},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143859},
  url = {http://doi.acm.org/10.1145/1143844.1143859},
  abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
  isbn = {978-1-59593-383-6}
}

@inproceedings{blei_latent_2001,
  title = {Latent {{Dirichlet Allocation}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Neural Information Processing Systems}}: {{Natural}} and {{Synthetic}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2001},
  series = {{{NIPS}}'01},
  pages = {601--608},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=2980539.2980618},
  abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof-mann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.}
}

@article{blei_latent_2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y and Jordan, Michael I},
  date = {2003-01},
  journaltitle = {Journal of machine Learning research},
  volume = {3},
  pages = {993--1022}
}

@unpublished{blei_nested_2007,
  title = {The Nested {{Chinese}} Restaurant Process and {{Bayesian}} Nonparametric Inference of Topic Hierarchies},
  author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
  date = {2007-10-03},
  eprint = {0710.0845},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/0710.0845},
  abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/franzilow/Zotero/storage/QF33HDD7/Blei et al. - 2007 - The nested Chinese restaurant process and Bayesian.pdf;/Users/franzilow/Zotero/storage/GC56ZTK4/0710.html}
}

@article{blei_probabilistic_2012,
  title = {Probabilistic {{Topic Models}}},
  author = {Blei, David M.},
  date = {2012-04},
  journaltitle = {Commun. ACM},
  volume = {55},
  number = {4},
  pages = {77--84},
  issn = {0001-0782},
  doi = {10.1145/2133806.2133826},
  url = {http://doi.acm.org/10.1145/2133806.2133826},
  abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
  file = {/Users/franzilow/Zotero/storage/GWUDPWQT/Blei - 2012 - Probabilistic Topic Models.pdf}
}

@incollection{boogaart_linear_2013,
  title = {Linear {{Models}} for {{Compositions}}},
  booktitle = {Analyzing {{Compositional Data}} with {{R}}},
  author = {family=Boogaart, given=K. Gerald, prefix=van den, useprefix=false and Tolosana-Delgado, Raimon},
  date = {2013},
  series = {Use {{R}}!},
  pages = {95--175},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36809-7_5},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-36809-7_5},
  urldate = {2017-11-29},
  abstract = {Compositions can play the role of dependent and independent variables in linear models. In both cases, the parameters of the linear models are again compositions of the same simplex as the data. Most methods for classical linear models have a close analog in these compositional linear models. This chapter addresses several questions on this subject. What are compositional linear models? How to visualize the dependence of compositions, already multivariable, with further external covariables? How to model and check such dependence with compositional linear models? What are the underlying assumptions? How can we check these assumptions? What is the compositional interpretation of the results? How to use linear models to provide statistical evidence with tests, confidence intervals, and predictive regions? How to visualize model results and model parameters? How to compare compositional linear models and how to find the most appropriate one?},
  isbn = {978-3-642-36808-0 978-3-642-36809-7},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/XXRU4BZK/Boogaart und Tolosana-Delgado - 2013 - Linear Models for Compositions.pdf;/Users/franzilow/Zotero/storage/66BBBZV2/978-3-642-36809-7_5.html}
}

@article{boomgaarden_nachrichtenbias_2012,
  title = {Nachrichten-{{Bias}}: {{Medieninhalte}}, {{Bevölkerungswahrnehmungen}} Und {{Wahlentscheidungen}} Bei Der {{Bundestagswahl}} 2009},
  shorttitle = {Nachrichten-{{Bias}}},
  author = {Boomgaarden, H. G. and Semetko, H. A.},
  date = {2012},
  journaltitle = {Politische Vierteljahresschrift. Sonderheft},
  pages = {442--464},
  url = {https://dare.uva.nl/search?identifier=abe81e9d-edf1-49d4-af5b-9c06183a576e},
  urldate = {2018-10-20},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/R2USHNUR/search.html}
}

@article{boomgaarden_reporting_2007,
  title = {Reporting {{Germany}}'s 2005 {{Bundestag Election Campaign}}: {{Was Gender}} an {{Issue}}?},
  author = {Boomgaarden, Hajo G. and Semetko, Holli A.},
  date = {2007-10-01},
  journaltitle = {Harvard International Journal of Press/Politics},
  volume = {12},
  number = {4},
  pages = {154--171}
}

@article{boumans_taking_2016,
  title = {Taking {{Stock}} of the {{Toolkit}}},
  author = {Boumans, Jelle W. and Trilling, Damian},
  date = {2016-01-02},
  journaltitle = {Digital Journalism},
  volume = {4},
  number = {1},
  pages = {8--23},
  issn = {2167-0811},
  doi = {10.1080/21670811.2015.1096598},
  url = {https://doi.org/10.1080/21670811.2015.1096598},
  urldate = {2018-10-11},
  abstract = {When analyzing digital journalism content, journalism scholars are confronted with a number of substantial differences compared to traditional journalistic content. The sheer amount of data and the unique features of digital content call for the application of valuable new techniques. Various other scholarly fields are already applying computational methods to study digital journalism data. Often, their research interests are closely related to those of journalism scholars. Despite the advantages that computational methods have over traditional content analysis methods, they are not commonplace in digital journalism studies. To increase awareness of what computational methods have to offer, we take stock of the toolkit and show the ways in which computational methods can aid journalism studies. Distinguishing between dictionary-based approaches, supervised machine learning, and unsupervised machine learning, we present a systematic inventory of recent applications both inside as well as outside journalism studies. We conclude with suggestions for how the application of new techniques can be encouraged.},
  keywords = {automated content analysis,computational social science,digital data,journalism studies,review},
  file = {/Users/franzilow/Zotero/storage/JAPM52RU/21670811.2015.html}
}

@article{brandenburg_party_2006,
  title = {Party {{Strategy}} and {{Media Bias}}: {{A Quantitative Analysis}} of the 2005 {{UK Election Campaign}}},
  shorttitle = {Party {{Strategy}} and {{Media Bias}}},
  author = {Brandenburg, Heinz},
  date = {2006-07-01},
  journaltitle = {Journal of Elections, Public Opinion and Parties},
  volume = {16},
  number = {2},
  pages = {157--178},
  issn = {1745-7289},
  doi = {10.1080/13689880600716027},
  url = {https://doi.org/10.1080/13689880600716027},
  urldate = {2018-09-25},
  abstract = {This article investigates the current state of press partisanship in the UK. Utilizing content analysis data from the 2005 General Election campaign, recent hypotheses about press dealignment are tested with quantitative methods. Partisan tendencies in reporting are measured in terms of coverage bias, statement bias, and agenda bias. As the governing party, Labour benefits from coverage bias in all papers, while the Liberal Democrats remain marginalized. It can be shown that increasingly ambiguous endorsements in broadsheet and tabloid press alike translate into a general absence of open support for political parties. At best, endorsed parties receive neutral treatment, with their opponents being harshly criticized. Partisan tendencies do, however, manifest themselves in other patterns of campaign coverage. Even weakly partisan papers engage in strategic behaviour, most notably by reinforcing the issue agendas of endorsed parties. With both the Independent and the Guardian lending strategic support to the Liberal Democrats, and the Murdoch press being largely non‐committal, the analysis hints at an erosion of support for New Labour.},
  file = {/Users/franzilow/Zotero/storage/DJPIC8M4/13689880600716027.html}
}

@article{brandenburg_political_2005,
  title = {Political {{Bias}} in the {{Irish Media}}: {{A Quantitative Study}} of {{Campaign Coverage}} during the 2002 {{General Election}}},
  shorttitle = {Political {{Bias}} in the {{Irish Media}}},
  author = {Brandenburg, Heinz},
  date = {2005-09-01},
  journaltitle = {Irish Political Studies},
  volume = {20},
  number = {3},
  pages = {297--322},
  issn = {0790-7184},
  doi = {10.1080/07907180500359350},
  url = {https://doi.org/10.1080/07907180500359350},
  urldate = {2018-10-20},
  abstract = {The aim of this study is to give some systematic insights into how Irish media tend to report an election campaign. The main focus will be on their attitudes toward and treatment of the competing parties and candidates. Content analysis data from television newscasts and campaign stories in four of the largest newspapers is used to investigate three different forms of media bias: coverage bias, agenda bias, and statement bias. We find that Irish media tend to grant disproportionate amounts of coverage to the government parties, Fianna Fáil and the Progressive Democrats; the more prominent the coverage, the less proportionate it becomes. The extent to which media take the freedom to ‘distort’ party agendas in their reporting appears to depend on party size, campaign strategy and the acquired status and acceptance of a party amongst the political and media establishment. Most notable, however, is the predominantly negative attitude of all Irish print media towards political actors. Instead of a polarised partisan press, as for example in the UK, in Ireland we seem to be faced with a rather homogenous anti‐politics bias.},
  file = {/Users/franzilow/Zotero/storage/TS2G9CU6/07907180500359350.html}
}

@article{braun_variational_2010,
  title = {Variational Inference for Large-Scale Models of Discrete Choice},
  author = {Braun, Michael and McAuliffe, Jon},
  date = {2010-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {105},
  number = {489},
  eprint = {0712.2526},
  eprinttype = {arxiv},
  pages = {324--335},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2009.tm08030},
  url = {http://arxiv.org/abs/0712.2526},
  urldate = {2018-01-19},
  abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/franzilow/Zotero/storage/NWRMRPXH/Braun und McAuliffe - 2010 - Variational inference for large-scale models of di.pdf;/Users/franzilow/Zotero/storage/V373IACQ/0712.html}
}

@inproceedings{buntine_estimating_2009,
  title = {Estimating {{Likelihoods}} for {{Topic Models}}},
  booktitle = {Proceedings of the 1st {{Asian Conference}} on {{Machine Learning}}: {{Advances}} in {{Machine Learning}}},
  author = {Buntine, Wray},
  date = {2009},
  series = {{{ACML}} '09},
  pages = {51--64},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-05224-8_6},
  url = {http://dx.doi.org/10.1007/978-3-642-05224-8_6},
  abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {$<$}em{$>$}topic{$<$}/em{$>$} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
  isbn = {978-3-642-05223-1}
}

@book{cage_information_2017,
  title = {L'information à tout prix},
  author = {Cagé, Julia and Hervé, Nicolas and Viaud, Marie-Luce},
  date = {2017},
  publisher = {{Éditions de l’INA}},
  langid = {french},
  file = {/Users/franzilow/Zotero/storage/E5Z7FALC/Lire-L-information-a-tout-prix-de-Julia-Cage.html}
}

@article{caillaud_chicken_2003,
  title = {Chicken \& {{Egg}}: {{Competition}} among {{Intermediation Service Providers}}},
  shorttitle = {Chicken \& {{Egg}}},
  author = {Caillaud, Bernard and Jullien, Bruno},
  date = {2003},
  journaltitle = {RAND Journal of Economics},
  volume = {34},
  number = {2},
  pages = {309--28},
  issn = {0741-6261},
  url = {http://econpapers.repec.org/article/rjerandje/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.htm},
  urldate = {2016-10-18},
  abstract = {We analyze a model of imperfect price competition between intermediation service providers. We insist on features that are relevant for informational intermediation via the Internet: the presence of indirect network externalities, the possibility of using the nonexclusive services of several intermediaries, and the widespread practice of price discrimination based on users' identity and on usage. Efficient market structures emerge in equilibrium, as well as some specific form of inefficient structures. Intermediaries have incentives to propose non-exclusive services, as this moderates competition and allows them to exert market power. We analyze in detail the pricing and business strategies followed by intermediation services providers. Copyright 2003 by the RAND Corporation.},
  file = {/Users/franzilow/Zotero/storage/GD5R58G2/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.html}
}

@inproceedings{chaney_visualizing_2012,
  title = {Visualizing {{Topic Models}}},
  booktitle = {Sixth {{International AAAI Conference}} on {{Weblogs}} and {{Social Media}}},
  author = {Chaney, Allison June-Barlow and Blei, David M.},
  date = {2012-05-20},
  url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/view/4645},
  urldate = {2018-01-23},
  abstract = {Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools\&mdash;a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method.},
  eventtitle = {Sixth {{International AAAI Conference}} on {{Weblogs}} and {{Social Media}}},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/968W9M2F/Chaney und Blei - 2012 - Visualizing Topic Models.pdf;/Users/franzilow/Zotero/storage/DXRG63IG/4645.html}
}

@incollection{chang_reading_2009,
  title = {Reading {{Tea Leaves}}: {{How Humans Interpret Topic Models}}},
  shorttitle = {Reading {{Tea Leaves}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  date = {2009},
  pages = {288--296},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
  file = {/Users/franzilow/Zotero/storage/UUPBMREK/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf;/Users/franzilow/Zotero/storage/68XT476P/3700-reading-tea-leaves-how-humans-interpret-topic-models.html}
}

@article{corden_maximisation_1952,
  title = {The {{Maximisation}} of {{Profit}} by a {{Newspaper}}},
  author = {Corden, W. M.},
  date = {1952},
  journaltitle = {The Review of Economic Studies},
  volume = {20},
  number = {3},
  pages = {181--190},
  issn = {0034-6527},
  doi = {10.2307/2295888},
  url = {http://www.jstor.org/stable/2295888}
}

@article{dalessio_media_2000,
  title = {Media {{Bias}} in {{Presidential Elections}}: {{A Meta-Analysis}}},
  shorttitle = {Media {{Bias}} in {{Presidential Elections}}},
  author = {D'Alessio, Dave and Allen, Mike},
  date = {2000-12-01},
  journaltitle = {Journal of Communication},
  shortjournal = {J Commun},
  volume = {50},
  number = {4},
  pages = {133--156},
  issn = {0021-9916},
  doi = {10.1111/j.1460-2466.2000.tb02866.x},
  url = {https://academic.oup.com/joc/article/50/4/133/4110147},
  urldate = {2018-08-14},
  abstract = {Abstract.  A meta-analysis considered 59 quantitative studies containing data concerned with partisan media bias in presidential election campaigns since 1948.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/Q67V2BQV/D'Alessio und Allen - 2000 - Media Bias in Presidential Elections A Meta-Analy.pdf;/Users/franzilow/Zotero/storage/PW67GGZB/4110147.html}
}

@article{damstra_impact_2019,
  title = {The Impact of Immigration News on Anti-Immigrant Party Support: Unpacking Agenda-Setting and Issue Ownership Effects over Time},
  shorttitle = {The Impact of Immigration News on Anti-Immigrant Party Support},
  author = {Damstra, Alyt and Jacobs, Laura and Boukes, Mark and Vliegenthart, Rens},
  date = {2019-04-22},
  journaltitle = {Journal of Elections, Public Opinion and Parties},
  shortjournal = {Journal of Elections, Public Opinion and Parties},
  pages = {1--22},
  doi = {10.1080/17457289.2019.1607863},
  abstract = {This study focuses on immigration news as driver of anti-immigrant party support. Predicting the poll results of the Dutch Freedom Party (PVV), we distinguish between (a) general immigration news, news in which immigration is linked to (b) crime, (c) terrorism, or (d) the economy, and (e) immigration news in which the Freedom Party is mentioned explicitly. Focusing on the period 2004–2017, an extensive dataset has been generated using computer-assisted content analyses of media coverage (n = 1,697,976), public opinion data, and real-world indicators. Time-series analyses demonstrate a positive impact of general immigration news on anti-immigrant party support – and not on the support for other parties. No media effects are found for the immigration sub-issues or the visibility of the Freedom Party in the news. However, the main competitor on the right side of the political spectrum – the Liberal Party (VVD) – may also benefit from immigration news but only when it manages to be linked to the issue in the media coverage. Our results raise relevant questions about the preconditions of issue ownership effects, and show how issue ownership is a highly contested political asset in a fragmented and competitive multiparty system.},
  file = {/Users/franzilow/Zotero/storage/4JWBWAEK/Damstra et al. - 2019 - The impact of immigration news on anti-immigrant p.pdf}
}

@article{das_trends_2017,
  title = {Trends in {{Transportation Research}}},
  author = {Das, Subasish and Dixon, Karen and Sun, Xiaoduan and Dutta, Anandi and Zupancich, Michelle},
  date = {2017-01-01},
  journaltitle = {Transportation Research Record: Journal of the Transportation Research Board},
  shortjournal = {Transportation Research Record: Journal of the Transportation Research Board},
  volume = {2614},
  pages = {27--38},
  issn = {0361-1981},
  doi = {10.3141/2614-04},
  url = {http://trrjournalonline.trb.org/doi/abs/10.3141/2614-04},
  abstract = {Proceedings of journal and conference papers are good sources of big textual data to examine research trends in various branches of science. The contents, usually unstructured in nature, require fast machine-learning algorithms to be deciphered. Exploratory analysis through text mining usually provides the descriptive nature of the contents but lacks quantification of the topics and their correlations. Topic models are algorithms designed to discover the main theme or trend in massive collections of unstructured documents. Through the use of a structural topic model, an extension of latent Dirichlet allocation, this study introduced distinct topic models on the basis of the relative frequencies of the words used in the abstracts of 15,357 TRB compendium papers. With data from 7 years (2008 through 2014) of TRB annual meeting compendium papers, the 20 most dominant topics emerged from a bag of 4 million words. The findings of this study contributed to the understanding of topical trends in the complex and evolving field of transportation engineering research.},
  file = {/Users/franzilow/Zotero/storage/FTII7NXR/2614-04.html}
}

@article{deerwester_indexing_1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  date = {1990},
  journaltitle = {Journal of the American Society for Information Science},
  volume = {41},
  number = {6},
  pages = {391--407},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.},
  file = {/Users/franzilow/Zotero/storage/EVGTHKF3/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf;/Users/franzilow/Zotero/storage/5KT8CZXC/summary.html}
}

@report{dellavigna_fox_2006,
  type = {Working Paper},
  title = {The {{Fox News Effect}}: {{Media Bias}} and {{Voting}}},
  shorttitle = {The {{Fox News Effect}}},
  author = {DellaVigna, Stefano and Kaplan, Ethan},
  date = {2006-04},
  number = {12169},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w12169},
  url = {http://www.nber.org/papers/w12169},
  urldate = {2018-08-22},
  abstract = {Does media bias affect voting? We address this question by looking at the entry of Fox News in cable markets and its impact on voting. Between October 1996 and November 2000, the conservative Fox News Channel was introduced in the cable programming of 20 percent of US towns. Fox News availability in 2000 appears to be largely idiosyncratic. Using a data set of voting data for 9,256 towns, we investigate if Republicans gained vote share in towns where Fox News entered the cable market by the year 2000. We find a significant effect of the introduction of Fox News on the vote share in Presidential elections between 1996 and 2000. Republicans gain 0.4 to 0.7 percentage points in the towns which broadcast Fox News. The results are robust to town-level controls, district and county fixed effects, and alternative specifications. We also find a significant effect of Fox News on Senate vote share and on voter turnout. Our estimates imply that Fox News convinced 3 to 8 percent of its viewers to vote Republican. We interpret the results in light of a simple model of voter learning about media bias and about politician quality. The Fox News effect could be a temporary learning effect for rational voters, or a permanent effect for voters subject to non-rational persuasion.},
  file = {/Users/franzilow/Zotero/storage/NHXK22WD/DellaVigna und Kaplan - 2006 - The Fox News Effect Media Bias and Voting.pdf}
}

@article{dellavigna_fox_2007,
  title = {The {{Fox News Effect}}: {{Media Bias}} and {{Voting}}},
  shorttitle = {The {{Fox News Effect}}},
  author = {DellaVigna, Stefano and Kaplan, Ethan},
  date = {2007-08-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {Q J Econ},
  volume = {122},
  number = {3},
  pages = {1187--1234},
  issn = {0033-5533},
  doi = {10.1162/qjec.122.3.1187},
  url = {https://academic.oup.com/qje/article/122/3/1187/1879517},
  urldate = {2019-01-11},
  abstract = {Abstract.  Does media bias affect voting? We analyze the entry of Fox News in cable markets and its impact on voting. Between October 1996 and November 2000, th},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/MFV7TPM5/DellaVigna and Kaplan - 2007 - The Fox News Effect Media Bias and Voting.pdf;/Users/franzilow/Zotero/storage/65VZTT3U/1879517.html}
}

@article{devreese_valenced_2006,
  title = {Valenced News Frames and Public Support for the {{EU}}},
  author = {family=Vreese, given=Claes, prefix=de, useprefix=true and Boomgaarden, Hajo G.},
  date = {2006},
  journaltitle = {Communications},
  volume = {28},
  number = {4},
  pages = {361--381},
  issn = {1613-4087},
  doi = {10.1515/comm.2003.024},
  url = {https://www.degruyter.com/view/j/comm.2003.28.issue-4/comm.2003.024/comm.2003.024.xml},
  urldate = {2018-10-20},
  file = {/Users/franzilow/Zotero/storage/NKPLQKWU/2006 - Valenced news frames and public support for the EU.pdf}
}

@article{dewenter_can_2018,
  title = {Can {{Media Drive}} the {{Electorate}}? {{The Impact}} of {{Media Coverage}} on {{Party Affiliation}} and {{Voting Intentions}}},
  author = {Dewenter, Ralf and Linder, Melissa and Thomas, Tobias},
  date = {2018-04},
  journaltitle = {Working Paper Series, Helmut Schmidt University Hamburg, Department of Economics},
  volume = {179}
}

@book{dewenter_einfuhrung_2014,
  title = {Einführung in die neue Ökonomie der Medienmärkte: Eine wettbewerbsökonomische Betrachtung aus Sicht der Theorie der zweiseitigen Märkte},
  shorttitle = {Einführung in die neue Ökonomie der Medienmärkte},
  author = {Dewenter, Ralf and Rösch, Jürgen},
  date = {2014-10-14},
  eprint = {7uXSBAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer-Verlag}},
  abstract = {Das vorliegende Buch stellt erstmals die Theorie der zweiseitigen Märkte und deren Anwendung auf Medienmärkte intuitiv sowie modelltheoretisch dar. Nach einer Diskussion der ökonomischen Grundlagen werden relevante Modelle zweiseitiger Medienplattformen sowie Anwendungen für die Wettbewerbspolitik besprochen. Anschließend werden die wirtschaftspolitischen Implikationen der Theorie dargestellt. Anhand von realen Wettbewerbsfällen auf Internet-, Zeitungs- und Zeitschriftenmärkten wird diskutiert, ob und inwiefern Medienplattformen einer unterschiedlichen wettbewerbspolitischen und -rechtlichen Behandlung bedürfen. Das Buch dient damit sowohl den Studierenden der Wirtschaftswissenschaften und des Wettbewerbsrechts zum Verständnis der modernen Medienökonomik. Es gibt aber ebenso Hinweise für die wettbewerbspolitische Analyse von Medienmärkten in der Fallpraxis.},
  isbn = {978-3-658-04736-8},
  langid = {german},
  pagetotal = {273},
  keywords = {Business & Economics / General,Business & Economics / Industries / Media & Communications,Business & Economics / Management,Political Science / Public Policy / Economic Policy}
}

@article{druckman_impact_2005,
  title = {The {{Impact}} of {{Media Bias}}: {{How Editorial Slant Affects Voters}}},
  shorttitle = {The {{Impact}} of {{Media Bias}}},
  author = {Druckman, James N. and Parkin, Michael},
  date = {2005-11-01},
  journaltitle = {The Journal of Politics},
  shortjournal = {The Journal of Politics},
  volume = {67},
  number = {4},
  pages = {1030--1049},
  issn = {0022-3816},
  doi = {10.1111/j.1468-2508.2005.00349.x},
  url = {https://www.journals.uchicago.edu/doi/full/10.1111/j.1468-2508.2005.00349.x},
  urldate = {2018-09-25},
  abstract = {We investigate how editorial slant—defined as the quantity and tone of a newspaper's candidate coverage as influenced by its editorial position—shapes candidate evaluations and vote choice. We avoid various methodological pitfalls by focusing on a single Senate campaign in a single market with two competing, editorially distinct newspapers. Combining comprehensive content analyses of the papers with an Election Day exit poll, we assess the slant of campaign coverage and its effects on voters. We find compelling evidence that editorial slant influences voters’ decisions. Our results raise serious questions about the media's place in democratic processes.},
  file = {/Users/franzilow/Zotero/storage/PURKC6GV/Druckman und Parkin - 2005 - The Impact of Media Bias How Editorial Slant Affe.pdf;/Users/franzilow/Zotero/storage/V5PB5D3R/j.1468-2508.2005.00349.html}
}

@article{duru_relevance_2018,
  title = {{{THE RELEVANCE OF AGENDA-SETTING THEORY IN TWENTY FIRST CENTURY JOURNALISM PRACTISE}}},
  author = {Duru, Chike Walter},
  date = {2018-06-30},
  journaltitle = {International Journal of Social Sciences and Humanities Review},
  volume = {8},
  number = {2},
  issn = {2276-8645},
  url = {http://ijsshr.com/journal/index.php/IJSSHR/article/view/423},
  urldate = {2018-08-14},
  abstract = {This paper examines the relevance of Agenda Setting theory in twenty first century Journalism practice. Agenda-setting theory describes the capacity of news media to influence and guide public discourse. That is, if a news item is covered frequently and prominently, the audience will regard the issue as more important. Agenda-setting theory was formally developed by Dr. Max McCombs and Dr. Donald Shaw in a study on the 1968 presidential election. In the 1968 ‘Chapel Hill study,’ McCombs and Shaw demonstrated a strong correlation between what 100 residents of Chapel Hill, North Carolina thought was the most important election issue and what the local and national news media reported was the most important issue. By comparing the salience of issues in news content with the public's perceptions of the most important election issue, McCombs and Shaw were able to determine the degree to which the media determines public opinion. Since the 1968 study, published in a 1972 edition of Public Opinion Quarterly, more than 400 studies have been published on the agenda-setting function of the mass media. The argument that the Agenda-Setting Theory is still very relevant in contemporary Journalism practice was advanced in the discourse. Based on the Social Responsibility and Gate- Keeping Theories, the paper focused on the Agenda Setting Theory of Communication, its implications, applications and continued relevance, highlighting the functions of the mass media. It is recommended that media stakeholders do more to ensure that issues that will enhance the socio- politico- economic well-being of the people are hyped regularly, to ensure the achievement of set objectives.},
  langid = {english},
  keywords = {Agenda Setting Theory,Journalism,Mass Media,Twenty first Century},
  file = {/Users/franzilow/Zotero/storage/U3ZQAQHA/Duru - 2018 - THE RELEVANCE OF AGENDA-SETTING THEORY IN TWENTY F.pdf;/Users/franzilow/Zotero/storage/XRBMQKIQ/423.html}
}

@article{eberl_lying_2018,
  title = {Lying Press: {{Three}} Levels of Perceived Media Bias and Their Relationship with Political Preferences},
  shorttitle = {Lying Press},
  author = {Eberl, Jakob-Moritz},
  date = {2018-03-06},
  journaltitle = {Communications},
  shortjournal = {Communications},
  doi = {10.1515/commun-2018-0002},
  abstract = {In the context of decreasing media trust as well as the rise of populist movements in many Western Democracies, this study sets out to revisit the relationship between political preferences and perceived media bias. It investigates perceived bias of the entire media system, the perceived bias of individual outlets as well as perceived beneficiaries of this favorable coverage. Analyses are based on an online survey in Austria in 2015 (n \textasciitilde{} 1,679) and compare citizens’ perceived biases towards eight newspapers and television outlets. Results show that media system bias in Austria is strongly related to right-wing but not to left-wing extremism. Furthermore, there are not only differences between single outlets but also between media genres, as particularly tabloids are less afflicted by right-wing perceptions of bias. Finally, there is evidence of hostile media perceptions irrespective of actual media exposure.}
}

@article{eberl_one_2017,
  title = {One {{Bias Fits All}}? {{Three Types}} of {{Media Bias}} and {{Their Effects}} on {{Party Preferences}}},
  shorttitle = {One {{Bias Fits All}}?},
  author = {Eberl, Jakob-Moritz and Boomgaarden, Hajo G. and Wagner, Markus},
  date = {2017-12-01},
  journaltitle = {Communication Research},
  shortjournal = {Communication Research},
  volume = {44},
  number = {8},
  pages = {1125--1148},
  issn = {0093-6502},
  doi = {10.1177/0093650215614364},
  url = {https://doi.org/10.1177/0093650215614364},
  urldate = {2018-10-20},
  abstract = {Bias in political news coverage may have a profound influence on voter opinions and preferences. However, the concept of media bias actually encompasses different sub-types: Visibility bias is the salience of political actors, tonality bias the evaluation of these actors, and agenda bias the extent to which parties address preferred issues in media coverage. The present study is the first to explore how each type of bias influences party preferences. Using data from the Austrian parliamentary election campaign of 2013, we combine an online panel survey (n = 1,285) with measures of media bias from content analyses of party press releases (n = 1,922) and media coverage in eight newspapers (n = 6,970). We find substantial effects on party preferences for tonality bias and agenda bias, while visibility bias has no clear impact. Voters who are less politically sophisticated and lack a party identification are more susceptible to bias, and media bias can also reinforce existing partisan identities.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/X2Z8NJYK/Eberl et al. - 2017 - One Bias Fits All Three Types of Media Bias and T.pdf}
}

@article{egami_how_2017,
  title = {How to {{Make Causal Inferences Using Texts}}},
  author = {Egami, Naoki and Fong, Christian J. and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
  date = {2017-12-13},
  journaltitle = {Working Paper},
  file = {/Users/franzilow/Zotero/storage/LVA68CVK/how-make-causal-inferences-using-texts.html}
}

@article{ellman_what_2009,
  title = {What Do the {{Papers Sell}}? {{A Model}} of {{Advertising}} and {{Media Bias}}*},
  shorttitle = {What Do the {{Papers Sell}}?},
  author = {Ellman, Matthew and Germano, Fabrizio},
  date = {2009-04-01},
  journaltitle = {The Economic Journal},
  volume = {119},
  number = {537},
  pages = {680--704},
  issn = {1468-0297},
  doi = {10.1111/j.1468-0297.2009.02218.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02218.x/abstract},
  abstract = {We model the market for news as a two-sided market where newspapers sell news to readers who value accuracy and sell space to advertisers who value advert-receptive readers. In this setting, monopolistic newspapers under-report or bias news that sufficiently reduces advertiser profits. Paradoxically, increasing the size of advertising eventually leads competing newspapers to reduce advertiser bias. Nonetheless, advertisers can counter this effect if able to commit to news-sensitive cut-off strategies, potentially inducing as much bias as in the monopoly case. We use these results to explain contrasting historical and recent evidence on commercial bias and influence in the media.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/ESUGMQCA/Ellman und Germano - 2009 - What do the Papers Sell A Model of Advertising an.pdf;/Users/franzilow/Zotero/storage/E87BHVVH/abstract.html}
}

@article{endres_new_2003,
  title = {A New Metric for Probability Distributions},
  author = {Endres, D. M. and Schindelin, J. E.},
  date = {2003-07},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {49},
  number = {7},
  pages = {1858--1860},
  issn = {0018-9448},
  doi = {10.1109/TIT.2003.813506},
  abstract = {We introduce a metric for probability distributions, which is bounded, information-theoretically motivated, and has a natural Bayesian interpretation. The square root of the well-known χ2 distance is an asymptotic approximation to it. Moreover, it is a close relative of the capacitory discrimination and Jensen-Shannon divergence.},
  keywords = {Adaptive estimation,Algorithm design and analysis,asymptotic approximation,Bayes methods,Bayesian interpretation,Bayesian methods,bounded information-theoretically motivated metric,capacitory discrimination,Convergence,Gaussian noise,information theory,Iterative algorithms,Jensen-Shannon divergence,probability,Probability distribution,probability distributions,square root,Wavelet analysis,White noise,Writing,χ2 distance},
  file = {/Users/franzilow/Zotero/storage/H6L3EIL4/1207388.html}
}

@article{enikolopov_media_2011,
  title = {Media and {{Political Persuasion}}: {{Evidence}} from {{Russia}}},
  shorttitle = {Media and {{Political Persuasion}}},
  author = {Enikolopov, Ruben and Petrova, Maria and Zhuravskaya, Ekaterina},
  date = {2011},
  journaltitle = {The American Economic Review},
  volume = {101},
  number = {7},
  pages = {3253--3285},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/41408737},
  urldate = {2018-08-22},
  abstract = {This paper compares electoral outcomes of 1999 parliamentary elections in Russia among geographical areas with differential access to the only national TV channel independent from the government It was available to three-quarters of Russia's population and its signal availability was idiosyncratic, conditional on observables. Independent TV decreased aggregate vote for the government party by 8.9 percentage points, increased the combined vote for major opposition parties by 6.3 percentage points, and decreased turnout by 3.8 percentage points. The probability of voting for opposition parties increased for individuals who watched independent TV even controlling for voting intentions measured one month before elections.}
}

@article{erosheva_mixedmembership_2004,
  title = {Mixed-Membership Models of Scientific Publications},
  author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
  date = {2004-04-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {101},
  eprint = {15020766},
  eprinttype = {pmid},
  pages = {5220--5227},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307760101},
  url = {http://www.pnas.org/content/101/suppl_1/5220},
  urldate = {2017-10-12},
  abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
  issue = {suppl 1},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/QX4H27E9/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf;/Users/franzilow/Zotero/storage/NVF8AVJ7/5220.html}
}

@article{erosheva_mixedmembership_2004a,
  title = {Mixed-Membership Models of Scientific Publications},
  author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
  date = {2004-04-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {101},
  eprint = {15020766},
  eprinttype = {pmid},
  pages = {5220--5227},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307760101},
  url = {http://www.pnas.org/content/101/suppl_1/5220},
  urldate = {2018-01-19},
  abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
  issue = {suppl 1},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/EXNI8WAK/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf;/Users/franzilow/Zotero/storage/CXECBUCC/5220.html}
}

@inproceedings{esuli_sentiwordnet_2006,
  title = {{{SENTIWORDNET}}: {{A Publicly Available Lexical Resource}} for {{Opinion Mining}}},
  shorttitle = {{{SENTIWORDNET}}},
  booktitle = {In {{Proceedings}} of the 5th {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}’06},
  author = {Esuli, Andrea and Sebastiani, Fabrizio},
  date = {2006},
  pages = {417--422},
  abstract = {Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users' opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the "PN-polarity" of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.},
  file = {/Users/franzilow/Zotero/storage/5ZZV2UAA/Esuli und Sebastiani - 2006 - SENTIWORDNET A Publicly Available Lexical Resourc.pdf;/Users/franzilow/Zotero/storage/HP7P833M/summary.html}
}

@article{evans_economics_2008,
  title = {The {{Economics}} of the {{Online Advertising Industry}}},
  author = {Evans, David S.},
  date = {2008},
  journaltitle = {Review of Network Economics},
  volume = {7},
  number = {3},
  pages = {1--33},
  url = {https://ideas.repec.org/a/bpj/rneart/v7y2008i3n2.html},
  urldate = {2016-08-11},
  abstract = {Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
  file = {/Users/franzilow/Zotero/storage/Z6B7DBRS/v7y2008i3n2.html}
}

@article{evans_empirical_2003,
  title = {Some {{Empirical Aspects}} of {{Multi-sided Platform Industries}}},
  author = {Evans, David S.},
  date = {2003},
  journaltitle = {Review of Network Economics},
  volume = {2},
  number = {3},
  issn = {1446-9022},
  doi = {10.2202/1446-9022.1026},
  url = {https://www.degruyter.com/view/j/rne.2003.2.issue-3/rne.2003.2.3.1026/rne.2003.2.3.1026.xml},
  abstract = {Multi-sided platform markets have two or more different groups of customers that businesses have to get and keep on board to succeed. These industries range from dating clubs (men and women), to video game consoles (game developers and users), to payment cards (cardholders and merchants), to operating system software (application developers and users). They include some of the most important industries in the economy. A survey of businesses in these industries shows that multi-sided platform businesses devise entry strategies to get multiple sides of the market on board and devise pricing, product, and other competitive strategies to keep multiple customer groups on a common platform that internalizes externalities across members of these groups.},
  file = {/Users/franzilow/Zotero/storage/JXJD8ZCR/Evans - 2003 - Some Empirical Aspects of Multi-sided Platform Ind.pdf}
}

@report{evans_industrial_2005a,
  type = {Working Paper},
  title = {The {{Industrial Organization}} of {{Markets}} with {{Two-Sided Platforms}}},
  author = {Evans, David S. and Schmalensee, Richard},
  date = {2005-09},
  number = {11603},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w11603},
  url = {http://www.nber.org/papers/w11603},
  abstract = {Two-sided platforms (2SPs) cater to two or more distinct groups of customers, facilitating value-creating interactions between them. The village market and the village matchmaker were 2SPs; eBay and Match.com are more recent examples. Other examples include payment card systems, magazines, shopping malls, and personal computer operating systems. Building on the seminal work of Rochet and Tirole (2003), a rapidly growing literature has illuminated the economic principles that apply to 2SPs generally. One key result is that 2SPs may find it profit-maximizing to charge prices for one customer group that are below marginal cost or even negative, and such skewed pricing pattern is prevalent, although not universal, in industries that appear to be based on 2SPs. Over the years, courts have also recognized that certain industries, notably payment card systems and newspapers, now understood to be based on 2SPs, are governed by unusual economic relationships. This chapter provides an introduction to the economics of 2SPs and its application to several competition policy issues.},
  file = {/Users/franzilow/Zotero/storage/EWJINGI8/Evans und Schmalensee - 2005 - The Industrial Organization of Markets with Two-Si.pdf}
}

@article{farrell_corporate_2016,
  title = {Corporate Funding and Ideological Polarization about Climate Change},
  author = {Farrell, Justin},
  date = {2016-01-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {113},
  number = {1},
  eprint = {26598653},
  eprinttype = {pmid},
  pages = {92--97},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1509433112},
  url = {http://www.pnas.org/content/113/1/92},
  urldate = {2017-11-09},
  abstract = {Drawing on large-scale computational data and methods, this research demonstrates how polarization efforts are influenced by a patterned network of political and financial actors. These dynamics, which have been notoriously difficult to quantify, are illustrated here with a computational analysis of climate change politics in the United States. The comprehensive data include all individual and organizational actors in the climate change countermovement (164 organizations), as well as all written and verbal texts produced by this network between 1993–2013 (40,785 texts, more than 39 million words). Two main findings emerge. First, that organizations with corporate funding were more likely to have written and disseminated texts meant to polarize the climate change issue. Second, and more importantly, that corporate funding influences the actual thematic content of these polarization efforts, and the discursive prevalence of that thematic content over time. These findings provide new, and comprehensive, confirmation of dynamics long thought to be at the root of climate change politics and discourse. Beyond the specifics of climate change, this paper has important implications for understanding ideological polarization more generally, and the increasing role of private funding in determining why certain polarizing themes are created and amplified. Lastly, the paper suggests that future studies build on the novel approach taken here that integrates large-scale textual analysis with social networks.},
  langid = {english},
  keywords = {climate change,computational social science,funding,polarization,politics},
  file = {/Users/franzilow/Zotero/storage/8M3IS9E4/Farrell - 2016 - Corporate funding and ideological polarization abo.pdf;/Users/franzilow/Zotero/storage/9R5TQ8PC/92.html}
}

@article{ferree_four_2002,
  title = {Four {{Models}} of the {{Public Sphere}} in {{Modern Democracies}}},
  author = {Ferree, Myra Marx and Gamson, William A. and Gerhards, Jürgen and Rucht, Dieter},
  date = {2002},
  journaltitle = {Theory and Society},
  volume = {31},
  number = {3},
  pages = {289--324},
  issn = {0304-2421},
  url = {https://www.jstor.org/stable/658129},
  urldate = {2018-10-20}
}

@article{fu_analyzing_2013,
  title = {Analyzing {{Online Sentiment}} to {{Predict Telephone Poll Results}}},
  author = {Fu, King-wa and Chan, Chee-hon},
  date = {2013-09},
  journaltitle = {Cyberpsychology, Behavior, and Social Networking},
  volume = {16},
  number = {9},
  pages = {702--707},
  issn = {2152-2715, 2152-2723},
  doi = {10.1089/cyber.2012.0375},
  url = {http://online.liebertpub.com/doi/abs/10.1089/cyber.2012.0375},
  urldate = {2018-03-19},
  abstract = {The telephone survey is a common social science research method for capturing public opinion, for example, an individual’s values or attitudes, or the government’s approval rating. However, reducing domestic landline usage, increasing nonresponse rate, and suffering from response bias of the interviewee’s self-reported data pose methodological challenges to such an approach. Because of the labor cost of administration, a phone survey is often conducted on a biweekly or monthly basis, and therefore a daily reflection of public opinion is usually not available. Recently, online sentiment analysis of user-generated content has been deployed to predict public opinion and human behavior. However, its overall effectiveness remains uncertain. This study seeks to examine the temporal association between online sentiment reflected in social media content and phone survey poll results in Hong Kong. Specifically, it aims to find the extent to which online sentiment can predict phone survey results. Using autoregressive integrated moving average time-series analysis, this study suggested that online sentiment scores can lead phone survey results by about 8–15 days, and their correlation coefficients were about 0.16. The finding is significant to the study of social media in social science research, because it supports the conclusion that daily sentiment observed in social media content can serve as a leading predictor for phone survey results, keeping as much as 2 weeks ahead of the monthly announcement of opinion polls. We also discuss the practical and theoretical implications of this study.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/MPTLP2XU/Fu und Chan - 2013 - Analyzing Online Sentiment to Predict Telephone Po.pdf}
}

@incollection{gabszewicz_media_2015,
  title = {Media as Multi-Sided Platforms},
  booktitle = {Handbook on the Economics of the Media},
  author = {Gabszewicz, Jean Jaskold and Resende, Joana and Sonnac, Nathalie},
  date = {2015},
  series = {Handbook on the Economics of the Media. - {{Cheltenham}}, {{UK}} : {{Edward Elgar Publishing}}, {{ISBN}} 978-0-85793-888-6. - 2015, p. 3-35},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/MS8D937F/10011339834.html}
}

@article{gabszewicz_press_2001,
  title = {Press Advertising and the Ascent of the ‘{{Pensée Unique}}’},
  author = {Gabszewicz, Jean J. and Laussel, Dider and Sonnac, Nathalie},
  date = {2001-05-01},
  journaltitle = {European Economic Review},
  shortjournal = {European Economic Review},
  series = {15th {{Annual Congress}} of the {{European Economic Association}}},
  volume = {45},
  number = {4},
  pages = {641--651},
  issn = {0014-2921},
  doi = {10.1016/S0014-2921(01)00139-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0014292101001398},
  abstract = {The press industry depends in a crucial way on the possibility of financing an important fraction of its activities by advertising receipts. We show that this induces the editors of newspapers to moderate, in several cases, the political message they display to their readers, compared with the political opinions they would have expressed otherwise. To this end, we consider a three-stage game in which editors select sequentially their political image, the price of their newspaper and the advertising tariff they oppose to the advertisers. The intuition of the result lies in the fact that editors have to sell tasteless political messages to their readers in order to sell a larger audience to the advertisers.},
  keywords = {advertising,Channels-program diversity,TV-broadcasting},
  file = {/Users/franzilow/Zotero/storage/8RHUZD5J/Gabszewicz et al. - 2001 - Press advertising and the ascent of the ‘Pensée Un.pdf;/Users/franzilow/Zotero/storage/JH6EJC28/S0014292101001398.html}
}

@article{genkin_largescale_2007,
  title = {Large-{{Scale Bayesian Logistic Regression}} for {{Text Categorization}}},
  author = {Genkin, Alexander and Lewis, David D. and Madigan, David},
  date = {2007-08-01},
  journaltitle = {Technometrics},
  volume = {49},
  number = {3},
  pages = {291--304},
  issn = {0040-1706},
  doi = {10.1198/004017007000000245},
  url = {http://dx.doi.org/10.1198/004017007000000245},
  abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.},
  keywords = {Information retrieval,Lasso,Penalization,Ridge regression,Support vector classifier,Variable selection},
  file = {/Users/franzilow/Zotero/storage/9JI658VW/Genkin et al. - 2007 - Large-Scale Bayesian Logistic Regression for Text .pdf;/Users/franzilow/Zotero/storage/9XV4KG9V/004017007000000245.html}
}

@article{gentzkow_media_2004,
  title = {Media, {{Education}} and {{Anti-Americanism}} in the {{Muslim World}}},
  author = {Gentzkow, Matthew A. and Shapiro, Jesse M.},
  date = {2004-09},
  journaltitle = {Journal of Economic Perspectives},
  volume = {18},
  number = {3},
  pages = {117--133},
  issn = {0895-3309},
  doi = {10.1257/0895330042162313},
  url = {https://www.aeaweb.org/articles?id=10.1257/0895330042162313},
  urldate = {2019-01-11},
  abstract = {Recent surveys in the United States and the Muslim world show widespread misinformation about the events of September 11, 2001. Using data from 9 predominantly Muslim countries, we study how such beliefs depend on exposure to news media and levels of education. Standard economic theory would predict that increased access to information should cause beliefs to converge. More recent models of biased belief formation suggest that this result might hinge critically on who is providing the information. Consistent with the latter, we find that overall intensity of media use and level of education have at best a weak correlation with beliefs, while particular information sources have strong and divergent effects. Compared to those with little media exposure or schooling, individuals watching Arab news channels or educated in schools with little Western influence are less likely to agree that the September 11 attacks were carried out by Arab terrorists. Those exposed to media or education from Western sources are more likely to agree. Belief that the attacks were morally justified and general attitudes toward the US are also strongly correlated with source of information. These findings survive controls for demographic characteristics and are robust to identifying media effects using cross-country variation in language.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/DBBKM6N8/Gentzkow and Shapiro - 2004 - Media, Education and Anti-Americanism in the Musli.pdf;/Users/franzilow/Zotero/storage/5N7JWXRT/articles.html}
}

@article{gentzkow_media_2006,
  title = {Media {{Bias}} and {{Reputation}}},
  author = {Gentzkow, Matthew and Shapiro, Jesse~M.},
  date = {2006-04-01},
  journaltitle = {Journal of Political Economy},
  shortjournal = {Journal of Political Economy},
  volume = {114},
  number = {2},
  pages = {280--316},
  issn = {0022-3808},
  doi = {10.1086/499414},
  url = {https://www.journals.uchicago.edu/doi/10.1086/499414},
  urldate = {2019-03-30},
  abstract = {A Bayesian consumer who is uncertain about the quality of an information source will infer that the source is of higher quality when its reports conform to the consumer’s prior expectations. We use this fact to build a model of media bias in which firms slant their reports toward the prior beliefs of their customers in order to build a reputation for quality. Bias emerges in our model even though it can make all market participants worse off. The model predicts that bias will be less severe when consumers receive independent evidence on the true state of the world and that competition between independently owned news outlets can reduce bias. We present a variety of empirical evidence consistent with these predictions.},
  file = {/Users/franzilow/Zotero/storage/68DMKQDE/Gentzkow and Shapiro - 2006 - Media Bias and Reputation.pdf;/Users/franzilow/Zotero/storage/SHBPKLPN/499414.html}
}

@article{gentzkow_television_2006,
  title = {Television and {{Voter Turnout}}},
  author = {Gentzkow, Matthew},
  date = {2006-08-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {Q J Econ},
  volume = {121},
  number = {3},
  pages = {931--972},
  issn = {0033-5533},
  doi = {10.1162/qjec.121.3.931},
  url = {https://academic.oup.com/qje/article/121/3/931/1917885},
  urldate = {2019-01-19},
  abstract = {Abstract.  I use variation across markets in the timing of television's introduction to identify its impact on voter turnout. The estimated effect is significan},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/JK7HZM2M/Gentzkow - 2006 - Television and Voter Turnout.pdf;/Users/franzilow/Zotero/storage/RH435L8T/1917885.html}
}

@report{gentzkow_text_2017,
  type = {Working Paper},
  title = {Text as {{Data}}},
  author = {Gentzkow, Matthew and Kelly, Bryan T. and Taddy, Matt},
  date = {2017-03},
  number = {23276},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w23276},
  url = {http://www.nber.org/papers/w23276},
  abstract = {An ever increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.}
}

@article{gentzkow_what_2010,
  title = {What {{Drives Media Slant}}? {{Evidence From U}}.{{S}}. {{Daily Newspapers}}},
  shorttitle = {What {{Drives Media Slant}}?},
  author = {Gentzkow, Matthew and Shapiro, Jesse M.},
  date = {2010},
  journaltitle = {Econometrica},
  volume = {78},
  number = {1},
  pages = {35--71},
  issn = {1468-0262},
  doi = {10.3982/ECTA7195},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA7195},
  urldate = {2019-01-07},
  abstract = {We construct a new index of media slant that measures the similarity of a news outlet's language to that of a congressional Republican or Democrat. We estimate a model of newspaper demand that incorporates slant explicitly, estimate the slant that would be chosen if newspapers independently maximized their own profits, and compare these profit-maximizing points with firms' actual choices. We find that readers have an economically significant preference for like-minded news. Firms respond strongly to consumer preferences, which account for roughly 20 percent of the variation in measured slant in our sample. By contrast, the identity of a newspaper's owner explains far less of the variation in slant.},
  langid = {english},
  keywords = {Bias,media ownership,text categorization},
  file = {/Users/franzilow/Zotero/storage/9S2AALCS/Gentzkow and Shapiro - 2010 - What Drives Media Slant Evidence From U.S. Daily .pdf;/Users/franzilow/Zotero/storage/5AGBX3N8/ECTA7195.html}
}

@article{gerber_does_2009,
  title = {Does the {{Media Matter}}? {{A Field Experiment Measuring}} the {{Effect}} of {{Newspapers}} on {{Voting Behavior}} and {{Political Opinions}}},
  shorttitle = {Does the {{Media Matter}}?},
  author = {Gerber, Alan S. and Karlan, Dean and Bergan, Daniel},
  date = {2009-04},
  journaltitle = {American Economic Journal: Applied Economics},
  volume = {1},
  number = {2},
  pages = {35--52},
  issn = {1945-7782},
  doi = {10.1257/app.1.2.35},
  url = {https://www.aeaweb.org/articles?id=10.1257/app.1.2.35},
  urldate = {2019-01-11},
  abstract = {We conducted a field experiment to measure the effect of exposure to newspapers on political behavior and opinion. Before the 2005 Virginia gubernatorial election, we randomly assigned individuals to a Washington Post free subscription treatment, a Washington Times free subscription treatment, or a control treatment. We find no effect of either paper on political knowledge, stated opinions, or turnout in post-election survey and voter data. However, receiving either paper led to more support for the Democratic candidate, suggesting that media slant mattered less in this case than media exposure. Some evidence from voting records also suggests that receiving either paper led to increased 2006 voter turnout. (JEL D72, L82)},
  langid = {english},
  keywords = {Media,Models of Political Processes: Rent-seeking; Elections; Legislatures; and Voting Behavior; Entertainment},
  file = {/Users/franzilow/Zotero/storage/9HHDV4FA/Gerber et al. - 2009 - Does the Media Matter A Field Experiment Measurin.pdf;/Users/franzilow/Zotero/storage/ICJ94SJT/articles.html}
}

@inproceedings{godbole_largescale_2007,
  title = {Large-{{Scale Sentiment Analysis}} for {{News}} and {{Blogs}} ({{System Demonstration}})},
  author = {Godbole, Namrata and Srinivasaiah, Manjunath and Skiena, Steven},
  date = {2007-01},
  pages = {2},
  eventtitle = {Proceedings of the {{International Conference}} on {{Weblogs}} and {{Social Media}}},
  file = {/Users/franzilow/Zotero/storage/AJPISQHX/Godbole et al. - Large-Scale Sentiment Analysis for News and Blogs .pdf}
}

@book{graber_processing_1984,
  title = {Processing the {{News}}: {{How People Tame}} the {{Information Tide}}},
  shorttitle = {Processing the {{News}}},
  author = {Graber, Doris Appel},
  date = {1984},
  eprint = {pKTZAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {{Longman Press}},
  location = {{New York}},
  isbn = {978-0-8191-9098-7},
  langid = {english},
  pagetotal = {320},
  keywords = {Social Science / General}
}

@report{grajzl_structural_2017,
  type = {SSRN Scholarly Paper},
  title = {A {{Structural Topic Model}} of the {{Features}} and the {{Cultural Origins}} of {{Bacon}}'s {{Ideas}}},
  author = {Grajzl, Peter and Murrell, Peter},
  date = {2017-10-20},
  number = {ID 2944816},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=2944816},
  urldate = {2017-11-07},
  abstract = {We use machine-learning methods to study the features and origins of the thought of Francis Bacon, a key figure in the development of a cultural paradigm that provided intellectual roots for modern economic development. We estimate a structural topic model, a state-of-the-art methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are central in the ideas usually associated with Bacon: inductive epistemology and fact-seeking. While Bacon's epistemology is strongly connected with his jurisprudence, fact-seeking is more isolated from Bacon's other intellectual pursuits. The utilitarian promise of science and the central organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him, a finding suggesting that these aspects of the 'Baconian' culture owed little to Bacon's own contributions. Bacon's use of different topics varies notably with intended audience and chosen medium.},
  keywords = {culture,Francis Bacon,knowledge,law,natural philosophy,politics,religion},
  file = {/Users/franzilow/Zotero/storage/B862TXNA/papers.html}
}

@article{griffiths_finding_2004,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  date = {2004-04-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {101},
  eprint = {14872004},
  eprinttype = {pmid},
  pages = {5228--5235},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307752101},
  url = {http://www.pnas.org/content/101/suppl_1/5228},
  urldate = {2017-10-12},
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  issue = {suppl 1},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/9P87NGIJ/Griffiths und Steyvers - 2004 - Finding scientific topics.pdf;/Users/franzilow/Zotero/storage/TX4TP3NE/5228.html}
}

@incollection{griffiths_hierarchical_2004,
  title = {Hierarchical {{Topic Models}} and the {{Nested Chinese Restaurant Process}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 16},
  author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
  editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
  date = {2004},
  pages = {17--24},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
  file = {/Users/franzilow/Zotero/storage/RC2JTMS3/Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf;/Users/franzilow/Zotero/storage/H6Z7JS5C/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html}
}

@article{griffiths_probabilistic_2002,
  title = {A Probabilistic Approach to Semantic Representation},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  date = {2002-01-01},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {24},
  number = {24},
  url = {https://escholarship.org/uc/item/44x9v7m7},
  urldate = {2017-11-16},
  file = {/Users/franzilow/Zotero/storage/6EQ8VAJR/Griffiths und Steyvers - 2002 - A probabilistic approach to semantic representatio.pdf;/Users/franzilow/Zotero/storage/76HFDXBQ/44x9v7m7.pdf}
}

@article{grimmer_bayesian_2010,
  title = {A {{Bayesian Hierarchical Topic Model}} for {{Political Texts}}: {{Measuring Expressed Agendas}} in {{Senate Press Releases}}},
  shorttitle = {A {{Bayesian Hierarchical Topic Model}} for {{Political Texts}}},
  author = {Grimmer, Justin},
  date = {2010},
  journaltitle = {Political Analysis},
  volume = {18},
  number = {1},
  pages = {1--35},
  url = {https://papers.ssrn.com/abstract=1541022},
  urldate = {2017-10-07},
  abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
  keywords = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases,Justin  Grimmer,SSRN},
  file = {/Users/franzilow/Zotero/storage/3QRF5PHE/papers.html}
}

@article{grimmer_text_2013,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon},
  date = {2013},
  journaltitle = {Political Analysis},
  volume = {21},
  pages = {267--297},
  file = {/Users/franzilow/Zotero/storage/FF9QRDJ4/text-data-promise-and-pitfalls-automatic-content-analysis-methods-political.html}
}

@article{groseclose_measure_2005,
  title = {A {{Measure}} of {{Media Bias}}},
  author = {Groseclose, Tim and Milyo, Jeffrey},
  date = {2005},
  journaltitle = {The Quarterly Journal of Economics},
  volume = {120},
  number = {4},
  pages = {1191--1237},
  issn = {0033-5533},
  url = {https://www.jstor.org/stable/25098770},
  urldate = {2019-01-07},
  abstract = {[We measure media bias by estimating ideological scores for several major media outlets. To compute this, we count the times that a particular media outlet cites various think tanks and policy groups, and then compare this with the times that members of Congress cite the same groups. Our results show a strong liberal bias: all of the news outlets we examine, except Fox News' Special Report and the Washington Times, received scores to the left of the average member of Congress. Consistent with claims made by conservative critics, CBS Evening News and the New York Times received scores far to the left of center. The most centrist media outlets were PBS NewsHour, CNN's Newsnight, and ABC's Good Morning America; among print outlets, USA Today was closest to the center. All of our findings refer strictly to news content; that is, we exclude editorials, letters, and the like.]}
}

@article{gustafsson_circulation_1978,
  title = {The Circulation Spiral and the Principle of Household Coverage},
  author = {Gustafsson, Karl Erik},
  date = {1978-01-01},
  journaltitle = {Scandinavian Economic History Review},
  volume = {26},
  number = {1},
  pages = {1--14},
  issn = {0358-5522},
  doi = {10.1080/03585522.1978.10407893},
  url = {https://doi.org/10.1080/03585522.1978.10407893},
  abstract = {The growth of oligopoly within the newspaper industry is a widespread phenomenon which has been examined by both researchers into the mass media and public enquiries into the press in a number of countries. Politicians recognise the development and want to modify the process of concentration, prevent newspaper closures, and even promote new ventures. Many western countries have taken measures to try to control forces bearing toward concentration in the newspaper industry. Such efforts, however, require a thorough knowledge of the market and its mechanism.},
  file = {/Users/franzilow/Zotero/storage/D8TJWJWK/03585522.1978.html}
}

@incollection{hagen_data_2018,
  title = {Data {{Analytics}} for {{Policy Informatics}}: {{The Case}} of {{E-Petitioning}}},
  shorttitle = {Data {{Analytics}} for {{Policy Informatics}}},
  booktitle = {Policy {{Analytics}}, {{Modelling}}, and {{Informatics}}},
  author = {Hagen, Loni and Harrison, Teresa M. and Dumas, Catherine L.},
  date = {2018},
  series = {Public {{Administration}} and {{Information Technology}}},
  pages = {205--224},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-61762-6_9},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-61762-6_9},
  urldate = {2017-11-09},
  abstract = {To contribute to the development of policy informatics, we discuss the benefits of analyzing electronic petitions (e-petitions), a form of citizen-government discourse with deep historic roots that has recently transitioned into a technologically-enabled and novel form of political communication. We begin by presenting a rationale for the analysis of e-petitions as a type of e-participation that can contribute to the development of public policy, provided that it is possible to analyze the large volumes of data produced in petitioning processes. From there we consider two data analytic strategies that offer promising approaches to the analysis of e-petitions and that lend themselves to the future creation of policy informatics tools. We discuss the application of topic modeling to the analysis of e-petition textual data to identify emergent topics of substantial concern to the public. We further propose the application of social network analysis to data related to the dynamics of petitioning processes, such as the social connections between petition initiators and signers, and tweets that solicit petition signatures in petitioning campaigns; both may be useful in revealing patterns of collective action. The paper concludes by reflecting on issues that should be brought to bear on the construction of policy informatics tools that make use of e-petitioning data.},
  isbn = {978-3-319-61761-9 978-3-319-61762-6},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/QBNNBEIJ/978-3-319-61762-6_9.html}
}

@article{hahn_identification_2001,
  title = {Identification and {{Estimation}} of {{Treatment Effects}} with a {{Regression-Discontinuity Design}}},
  author = {Hahn, Jinyong and Todd, Petra and Van der Klaauw, Wilbert},
  date = {2001},
  journaltitle = {Econometrica},
  volume = {69},
  number = {1},
  pages = {201--209},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  url = {https://www.jstor.org/stable/2692190},
  urldate = {2021-08-15}
}

@article{haixia_extracting_2016,
  title = {Extracting Topics of Computer Science Literature with LDA Model, Extracting Topics of Computer Science Literature with LDA Model},
  author = {Haixia, Yang and Baojun, Gao and Hanlin, Sun and Haixia, Yang and Baojun, Gao and Hanlin, Sun},
  date = {2016-12-20},
  journaltitle = {Data Analysis and Knowledge Discovery},
  shortjournal = {Data Analysis and Knowledge Discovery},
  volume = {32},
  number = {11},
  pages = {20--26},
  issn = {2096-3467},
  doi = {10.11925/infotech.1003-3513.2016.11.03},
  url = {http://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/abstract/abstract4288.shtml},
  urldate = {2017-11-09},
  langid = {cn},
  file = {/Users/franzilow/Zotero/storage/PR58B4CK/Haixia et al. - 2016 - Extracting Topics of Computer Science Literature w.pdf;/Users/franzilow/Zotero/storage/DDFB9U5P/abstract4288.html}
}

@book{hamilton_all_2004,
  title = {All the {{News That}}'s {{Fit}} to {{Sell}}: {{How}} the {{Market Transforms Information}} into {{News}}},
  shorttitle = {All the {{News That}}'s {{Fit}} to {{Sell}}},
  author = {Hamilton, James T.},
  date = {2004},
  publisher = {{Princeton University Press}},
  doi = {10.2307/j.ctt7smgs},
  url = {https://www.jstor.org/stable/j.ctt7smgs},
  urldate = {2019-01-19},
  abstract = {That market forces drive the news is not news. Whether a story appears in print, on television, or on the Internet depends on who is interested, its value to advertisers, the costs of assembling the details, and competitors' products. But in \emph{All the News That's Fit to Sell} , economist James Hamilton shows just how this happens. Furthermore, many complaints about journalism--media bias, soft news, and pundits as celebrities--arise from the impact of this economic logic on news judgments.  This is the first book to develop an economic theory of news, analyze evidence across a wide range of media markets on how incentives affect news content, and offer policy conclusions. Media bias, for instance, was long a staple of the news. Hamilton's analysis of newspapers from 1870 to 1900 reveals how nonpartisan reporting became the norm. A hundred years later, some partisan elements reemerged as, for example, evening news broadcasts tried to retain young female viewers with stories aimed at their (Democratic) political interests. Examination of story selection on the network evening news programs from 1969 to 1998 shows how cable competition, deregulation, and ownership changes encouraged a shift from hard news about politics toward more soft news about entertainers.  Hamilton concludes by calling for lower costs of access to government information, a greater role for nonprofits in funding journalism, the development of norms that stress hard news reporting, and the defining of digital and Internet property rights to encourage the flow of news. Ultimately, this book shows that by more fully understanding the economics behind the news, we will be better positioned to ensure that the news serves the public good.},
  isbn = {978-0-691-12367-7}
}

@article{hansen_shocking_2016,
  title = {Shocking Language: {{Understanding}} the Macroeconomic Effects of Central Bank Communication},
  shorttitle = {Shocking Language},
  author = {Hansen, Stephen and McMahon, Michael},
  date = {2016-03-01},
  journaltitle = {Journal of International Economics},
  shortjournal = {Journal of International Economics},
  series = {38th {{Annual NBER International Seminar}} on {{Macroeconomics}}},
  volume = {99},
  pages = {S114-S133},
  issn = {0022-1996},
  doi = {10.1016/j.jinteco.2015.12.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0022199615001828},
  urldate = {2018-03-07},
  abstract = {We explore how the multi-dimensional aspects of information released by the FOMC has effects on both market and real economic variables. Using tools from computational linguistics, we measure the information released by the FOMC on the state of economic conditions, as well as the guidance the FOMC provides about future monetary policy decisions. Employing these measures within a FAVAR framework, we find that shocks to forward guidance are more important than the FOMC communication of current economic conditions in terms of their effects on market and real variables. Nonetheless, neither communication has particularly strong effects on real economic variables.},
  keywords = {Communication,Monetary policy,Vector autoregression},
  file = {/Users/franzilow/Zotero/storage/AINER6IR/Hansen und McMahon - 2016 - Shocking language Understanding the macroeconomic.pdf;/Users/franzilow/Zotero/storage/39TT2SW9/S0022199615001828.html}
}

@article{hausman_regression_2018,
  title = {Regression {{Discontinuity}} in {{Time}}: {{Considerations}} for {{Empirical Applications}}},
  shorttitle = {Regression {{Discontinuity}} in {{Time}}},
  author = {Hausman, Catherine and Rapson, David S.},
  date = {2018},
  journaltitle = {Annual Review of Resource Economics},
  volume = {10},
  number = {1},
  pages = {533--552},
  doi = {10.1146/annurev-resource-121517-033306},
  url = {https://doi.org/10.1146/annurev-resource-121517-033306},
  urldate = {2021-08-15},
  abstract = {Recent empirical work in several economic fields, particularly environmental and energy economics, has adapted the regression discontinuity (RD) framework to applications where time is the running variable and treatment begins at a particular threshold in time. In this guide for practitioners, we discuss several features of this regression discontinuity in time framework that differ from the more standard cross-sectional RD framework. First, many applications (particularly in environmental economics) lack cross-sectional variation and are estimated using observations far from the temporal threshold. This common empirical practice is hard to square with the assumptions of a cross-sectional RD, which is conceptualized for an estimation bandwidth shrinking even as the sample size increases. Second, estimates may be biased if the time-series properties of the data are ignored (for instance, in the presence of an autoregressive process), or more generally if short-run and long-run effects differ. Finally, tests for sorting or bunching near the threshold are often irrelevant, making the framework closer to an event study than a regression discontinuity design. Based on these features and motivated by hypothetical examples using air quality data, we offer suggestions for the empirical researcher wishing to use the RD in time framework.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-resource-121517-033306}
}

@inproceedings{he_detecting_2009,
  title = {Detecting {{Topic Evolution}} in {{Scientific Literature}}: {{How Can Citations Help}}?},
  shorttitle = {Detecting {{Topic Evolution}} in {{Scientific Literature}}},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {He, Qi and Chen, Bi and Pei, Jian and Qiu, Baojun and Mitra, Prasenjit and Giles, Lee},
  date = {2009},
  series = {{{CIKM}} '09},
  pages = {957--966},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1645953.1646076},
  url = {http://doi.acm.org/10.1145/1645953.1646076},
  urldate = {2018-01-23},
  abstract = {Understanding how topics in scientific literature evolve is an interesting and important problem. Previous work simply models each paper as a bag of words and also considers the impact of authors. However, the impact of one document on another as captured by citations, one important inherent element in scientific literature, has not been considered. In this paper, we address the problem of understanding topic evolution by leveraging citations, and develop citation-aware approaches. We propose an iterative topic evolution learning framework by adapting the Latent Dirichlet Allocation model to the citation network and develop a novel inheritance topic model. We evaluate the effectiveness and efficiency of our approaches and compare with the state of the art approaches on a large collection of more than 650,000 research papers in the last 16 years and the citation network enabled by CiteSeerX. The results clearly show that citations can help to understand topic evolution better.},
  isbn = {978-1-60558-512-3},
  keywords = {citations,inheritance topic model,topic evolution}
}

@report{heinrich_parameter_2004,
  title = {Parameter Estimation for Text Analysis},
  author = {Heinrich, Gregor},
  date = {2004},
  abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of LDA models are discussed.},
  file = {/Users/franzilow/Zotero/storage/PTHWFSPM/Heinrich - 2004 - Parameter estimation for text analysis.pdf;/Users/franzilow/Zotero/storage/JZDEF9EX/summary.html}
}

@software{hlavac_stargazer_2018,
  title = {Stargazer: {{Well-Formatted Regression}} and {{Summary Statistics Tables}}. {{R}} Package Version 5.2.2},
  author = {Hlavac, Marek},
  date = {2018},
  url = {https://CRAN.R-project.org/package=stargazer}
}

@incollection{hoffman_online_2010,
  title = {Online {{Learning}} for {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
  editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
  date = {2010},
  pages = {856--864},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
  file = {/Users/franzilow/Zotero/storage/E3N5REJ7/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf;/Users/franzilow/Zotero/storage/NE5ZUD83/3902-online-learning-for-latent-dirichlet-allocation.html}
}

@inproceedings{hofmann_probabilistic_1999,
  title = {Probabilistic {{Latent Semantic Indexing}}},
  booktitle = {Proceedings of the {{22Nd Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Hofmann, Thomas},
  date = {1999},
  series = {{{SIGIR}} '99},
  pages = {50--57},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/312624.312649},
  url = {http://doi.acm.org/10.1145/312624.312649},
  isbn = {978-1-58113-096-6}
}

@inproceedings{holbert_new_2010,
  title = {A {{New Era}} of {{Minimal Effects}} ? {{A Response}} to {{Bennett}} and {{Iyengar}}},
  shorttitle = {A {{New Era}} of {{Minimal Effects}} ?},
  author = {Holbert, R. Lance and Garrett, R. Kelly and Gleason, Laurel S.},
  date = {2010},
  abstract = {This article takes up Bennett and Iyengar’s (2008) call for debate about the future of political communication effects research. We outline 4 key criticisms. First, Bennett and Iyengar are too quick to dismiss the importance of attitude reinforcement, long recognized as an important type of political media influence. Second, the authors take too narrow a view of the sources of political information, remaining fixated on news. Third, they offer an incomplete portrayal of selective exposure, exaggerating the extent to which individuals avoid attitudediscrepant information. Finally, they lean toward determinism when describing the role technologies play in shaping our political environment. In addition, we challenge Bennett and Iyengar’s assertion that only brand new theory can serve to help researchers understand today’s political communication landscape. We argue that existing tools, notably the Elaboration Likelihood Model (ELM), retain much utility for examining political media effects. Contrary to Bennett and Iyengar’s claims, the ELM suggests that the contemporary political information environment does not necessarily lead to minimal effects.},
  keywords = {Assertion (software development),Elaboration likelihood model,Noise shaping},
  file = {/Users/franzilow/Zotero/storage/6V2NVRW3/Holbert et al. - 2010 - A New Era of Minimal Effects  A Response to Benne.pdf}
}

@article{holig_reuters_2018,
  title = {Reuters {{Institute Digital News Report}} 2018 – {{Ergebnisse}} Für {{Deutschland}}},
  author = {Hölig, Sascha and Hasebrink, Uwe},
  date = {2018-06},
  journaltitle = {Arbeitspapiere des Hans‐Bredow‐ Instituts},
  volume = {44},
  issn = {1435‐9413}
}

@article{hopmann_political_2012,
  title = {Political Balance in the News: {{A}} Review of Concepts, Operationalizations and Key Findings},
  shorttitle = {Political Balance in the News},
  author = {Hopmann, David Nicolas and Van Aelst, Peter and Legnante, Guido},
  date = {2012-02-01},
  journaltitle = {Journalism},
  shortjournal = {Journalism},
  volume = {13},
  number = {2},
  pages = {240--257},
  issn = {1464-8849},
  doi = {10.1177/1464884911427804},
  url = {https://doi.org/10.1177/1464884911427804},
  urldate = {2018-11-10},
  abstract = {Balance is a notoriously difficult concept to operationalize. It has typically been investigated by examining the issues raised in elections, as well as the volume and favorability of coverage of political actors. However, even after collecting these measures, it is difficult to determine precisely what would constitute ‘balanced’ coverage. Based on a comprehensive overview of previous research in western democracies, we argue that political balance can be defined according to a political system perspective (where coverage reflects politically defined norms or regulation) or a media routine perspective (where coverage results from journalistic norms). Unless forced to follow norms, western broadcasting seems to comply with a media routine perspective. Empirically, newspaper coverage is sometimes imbalanced according to both perspectives. Finally, we discuss why only a systematic analysis of explanations across time and space makes it possible to determine whether politically ‘imbalanced’ news is the result of partisan bias or not.},
  langid = {english}
}

@article{hurtikova_importance_2017,
  title = {The {{Importance}} of {{Valence-Framing}} in the {{Process}} of {{Political Communicati}} on: {{Effects}} on the {{Formation}} of {{Political Attitudes}} among {{Viewers}} of {{Television News}} in the {{Czech Republic}} | {{Media Studies}}},
  shorttitle = {The {{Importance}} of {{Valence-Framing}} in the {{Process}} of {{Political Communicati}} On},
  author = {Hurtíková, Hanna},
  date = {2017-12-21},
  volume = {8},
  number = {15},
  url = {https://hrcak.srce.hr/ojs/index.php/medijske-studije/article/view/6200},
  urldate = {2018-11-16},
  langid = {american},
  file = {/Users/franzilow/Zotero/storage/RA2RTB5M/The Importance of Valence-Framing in the Process o.pdf;/Users/franzilow/Zotero/storage/76ZUI94X/6200.html}
}

@article{imbens_regression_2008,
  title = {Regression Discontinuity Designs: {{A}} Guide to Practice},
  shorttitle = {Regression Discontinuity Designs},
  author = {Imbens, Guido W. and Lemieux, Thomas},
  date = {2008-02-01},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  series = {The Regression Discontinuity Design: {{Theory}} and Applications},
  volume = {142},
  number = {2},
  pages = {615--635},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2007.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0304407607001091},
  urldate = {2021-08-15},
  abstract = {In regression discontinuity (RD) designs for evaluating causal effects of interventions, assignment to a treatment is determined at least partly by the value of an observed covariate lying on either side of a fixed threshold. These designs were first introduced in the evaluation literature by Thistlewaite and Campbell [1960. Regression-discontinuity analysis: an alternative to the ex-post Facto experiment. Journal of Educational Psychology 51, 309–317] With the exception of a few unpublished theoretical papers, these methods did not attract much attention in the economics literature until recently. Starting in the late 1990s, there has been a large number of studies in economics applying and extending RD methods. In this paper we review some of the practical and theoretical issues in implementation of RD methods.},
  langid = {english},
  keywords = {Nonparametric estimation,Regression discontinuity,Treatment effects},
  file = {/Users/franzilow/Zotero/storage/PR9W69RN/Imbens und Lemieux - 2008 - Regression discontinuity designs A guide to pract.pdf;/Users/franzilow/Zotero/storage/BDJVCEY8/S0304407607001091.html}
}

@book{jacob_practical_2012,
  title = {A {{Practical Guide}} to {{Regression Discontinuity}}},
  author = {Jacob, Robin and Zhu, Pei and Somers, Marie-Andrée and Bloom, Howard},
  date = {2012-07},
  journaltitle = {MDRC},
  publisher = {{MDRC}},
  url = {https://eric.ed.gov/?id=ED565862},
  urldate = {2021-08-15},
  abstract = {Regression discontinuity (RD) analysis is a rigorous nonexperimental approach that can be used to estimate program impacts in situations in which candidates are selected for treatment based on whether their value for a numeric rating exceeds a designated threshold or cut-point. Over the last two decades, the regression discontinuity approach has been used to evaluate the impact of a wide variety of social programs (DiNardo and Lee, 2004; Hahn, Todd, and van der Klaauw, 1999; Lemieux and Milligan, 2004; van der Klaauw, 2002; Angrist and Lavy, 1999; Jacob and Lefgren, 2006; McEwan and Shapiro, 2008; Black, Galdo, and Smith, 2007; Gamse, Bloom, Kemple, and Jacob, 2008). Yet, despite the growing popularity of the approach, there is only a limited amount of accessible information to guide researchers in the implementation of an RD design. While the approach is intuitively appealing, the statistical details regarding the implementation of an RD design are more complicated than they might first appear. Most of the guidance that currently exists appears in technical journals that require a high degree of technical sophistication to read. Furthermore, the terminology that is used is not well defined and is often used inconsistently. Finally, while a number of different approaches to the implementation of an RD design are proposed in the literature, they each differ slightly in their details. As such, even researchers with a fairly sophisticated statistical background can find it difficult to access practical guidance for the implementation of an RD design. To help fill this void, the present paper is intended to serve as a practitioners' guide to implementing RD designs. It seeks to explain things in easy-to-understand language and to offer best practices and general guidance to those attempting an RD analysis. In addition, the guide illustrates the various techniques available to researchers and explores their strengths and weaknesses using a simulated dataset. The guide provides a general overview of the RD approach and then covers the following topics in detail: (1) graphical presentation in RD analysis, (2) estimation (both parametric and nonparametric), (3) establishing the interval validity of RD impacts, (4) the precision of RD estimates, (5) the generalizability of RD findings, and (6) estimation and precision in the context of a fuzzy RD analysis. Readers will find both a glossary of widely used terms and a checklist of steps to follow when implementing an RD design in the Appendixes. The following are appended: (1) Glossary; (2) Checklists for Researchers; and (3) For Further Investigation.},
  langid = {english},
  keywords = {Accuracy,Computation,Generalizability Theory,Graphs,Nonparametric Statistics,Regression (Statistics),Research Design,Validity},
  file = {/Users/franzilow/Zotero/storage/VXN4GXQ8/Jacob et al. - 2012 - A Practical Guide to Regression Discontinuity.pdf;/Users/franzilow/Zotero/storage/WZXGHCDH/eric.ed.gov.html}
}

@report{jegadeesh_deciphering_2017,
  type = {SSRN Scholarly Paper},
  title = {Deciphering {{Fedspeak}}: {{The Information Content}} of {{FOMC Meetings}}},
  shorttitle = {Deciphering {{Fedspeak}}},
  author = {Jegadeesh, Narasimhan and Wu, Di (Andrew)},
  date = {2017-03-23},
  number = {ID 2939937},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=2939937},
  urldate = {2019-05-24},
  abstract = {We present a new approach to quantify the economic and policy content of the Federal Reserve communications by dissecting the Federal Open Market Committee (FOMC) meeting minutes into eight distinct economic topics. We examine the informativeness of the Fed's discussion of each of these topics for the stock market and for interest rates. The market finds the Fed's discussion of its policy stance, inflation and employment to be the most informative and its discussion of topics such as trade, consumption and investment are not informative.},
  langid = {english},
  keywords = {Deciphering Fedspeak: The Information Content of FOMC Meetings,Di (Andrew) Wu,Narasimhan  Jegadeesh,SSRN},
  file = {/Users/franzilow/Zotero/storage/RF5NUAUD/Jegadeesh and Wu - 2017 - Deciphering Fedspeak The Information Content of F.pdf;/Users/franzilow/Zotero/storage/RPBSQRNF/papers.html}
}

@software{johncoene_webhoser_2019,
  title = {Webhoser: {{R}} Wrapper for the Webhose.Io {{API}}},
  author = {{John Coene}},
  date = {2019-07-29T15:38:42Z},
  origdate = {2018-05-16T20:35:11Z},
  url = {https://github.com/news-r/webhoser},
  urldate = {2022-06-12},
  abstract = {R package version 0.0.1 --- For new features, see the 'Changelog' file (in the package source)},
  organization = {{news-r}},
  keywords = {api,news,r,rstats,webhoseio}
}

@article{junquedefortuny_evaluating_2014,
  title = {Evaluating and Understanding Text-Based Stock Price Prediction Models},
  author = {Junqué de Fortuny, Enric and De Smedt, Tom and Martens, David and Daelemans, Walter},
  date = {2014-03-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {50},
  number = {2},
  pages = {426--441},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2013.12.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0306457313001143},
  urldate = {2018-10-11},
  abstract = {Despite the fact that both the Efficient Market Hypothesis and Random Walk Theory postulate that it is impossible to predict future stock prices based on currently available information, recent advances in empirical research have been proving the opposite by achieving what seems to be better than random prediction performance. We discuss some of the (dis)advantages of the most widely used performance metrics and conclude that is difficult to assess the external validity of performance using some of these measures. Moreover, there remain many questions as to the real-world applicability of these empirical models. In the first part of this study we design novel stock price prediction models, based on state-of-the-art text-mining techniques to assert whether we can predict the movement of stock prices more accurately by including indicators of irrationality. Along with this, we discuss which metrics are most appropriate for which scenarios in order to evaluate the models. Finally, we discuss how to gain insight into text-mining-based stock price prediction models in order to evaluate, validate and refine the models.},
  keywords = {Evaluation,Prediction,Sentiment mining,Stock,Support Vector Machine},
  file = {/Users/franzilow/Zotero/storage/WENRITVN/S0306457313001143.html}
}

@article{junquedefortuny_media_2012,
  title = {Media Coverage in Times of Political Crisis: {{A}} Text Mining Approach},
  shorttitle = {Media Coverage in Times of Political Crisis},
  author = {Junqué de Fortuny, Enric and De Smedt, Tom and Martens, David and Daelemans, Walter},
  date = {2012-10-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {39},
  number = {14},
  pages = {11616--11622},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2012.04.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417412006100},
  urldate = {2018-08-14},
  abstract = {At the year end of 2011 Belgium formed a government, after a world record breaking period of 541days of negotiations. We have gathered and analysed 68,000 related on-line news articles published in 2011 in Flemish newspapers. These articles were analysed by a custom-built expert system. The results of our text mining analyses show interesting differences in media coverage and votes for several political parties and politicians. With opinion mining, we are able to automatically detect the sentiment of each article, thereby allowing to visualise how the tone of reporting evolved throughout the year, on a party, politician and newspaper level. Our suggested framework introduces a generic text mining approach to analyse media coverage on political issues, including a set of methodological guidelines, evaluation metrics, as well as open source opinion mining tools. Since all analyses are based on automated text mining algorithms, an objective overview of the manner of reporting is provided. The analysis shows peaks of positive and negative sentiments during key moments in the negotiation process.},
  keywords = {Coverage,Data mining,Opinion mining,Politics,Sentiment mining},
  file = {/Users/franzilow/Zotero/storage/KVJFUAGS/S0957417412006100.html}
}

@incollection{kepplinger_einfluss_2004,
  title = {Der {{Einfluss}} Der {{Pressemitteilungen}} Der {{Bundesparteien}} Auf Die {{Berichterstattung}} Im {{Bundestagswahlkampf}} 2002},
  booktitle = {Quo Vadis {{Public Relations}}? {{Auf}} Dem {{Weg}} Zum {{Kommunikationsmanagement}}: {{Bestandsaufnahmen}} Und {{Entwicklungen}}},
  author = {Kepplinger, Hans Mathias and Maurer, Marcus},
  editor = {Raupp, Juliana and Klewes, Joachim},
  date = {2004},
  pages = {113--124},
  publisher = {{VS Verlag für Sozialwissenschaften}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-322-83381-5_9},
  url = {https://doi.org/10.1007/978-3-322-83381-5_9},
  abstract = {Startschüsse sind notwendig, laut und sehen. Dies trifft auch auf die Studie zum Einfluss der Pressearbeit der Parteien in Nordrhein-Westfalen auf die Berichterstattung regionaler Tageszeitungen zu, die Barbara Baerns bereits 1981 abgeschlossen, jedoch erst später unter dem Titel „Öffentlichkeitsarbeit oder Joumalismus? Zum Einflu\textbackslash s s im Mediensystem“ (Baerns 1985) publizierte. Sie öffnete ein weites Forschungsfeld, das zwar viele geahnt und wortreich umschrieben haben, aber niemand auf breiter Basis empirisch angegangen war. Nicht neu, aber wegweisend war die Anlage der Untersuchung als Input-Output-Analyse, die allerdings auf einen Vergleich der Stuktur des Angebotes an PR-Meldungen mit der Stmktur der abgedmck- ten Angebote beschränkt blieb (Baerns 1985: 70-73). Spektakulär und viel zitiert war ihr Befund, dass knapp zwei Drittel der publizierten Beiträge auf PR-Quellen bemhten (Baerns 1985: 66-68). Er wurde zur Gmndlage der „Determiniemngs These“, die angesichts wachsender PR-Abteilungen und schmmpfender Redaktionen besondere Aktualität besitzt.},
  isbn = {978-3-322-83381-5}
}

@inproceedings{kim_topic_2011,
  title = {Topic {{Chains}} for {{Understanding}} a {{News Corpus}}},
  booktitle = {Computational {{Linguistics}} and {{Intelligent Text Processing}}},
  author = {Kim, Dongwoo and Oh, Alice},
  date = {2011-02-20},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {163--176},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-19437-5_13},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13},
  urldate = {2018-01-23},
  abstract = {The Web is a great resource and archive of news articles for the world. We present a framework, based on probabilistic topic modeling, for uncovering the meaningful structure and trends of important topics and issues hidden within the news archives on the Web. Central in the framework is a topic chain, a temporal organization of similar topics. We experimented with various topic similarity metrics and present our insights on how best to construct topic chains. We discuss how to interpret the topic chains to understand the news corpus by looking at long-term topics, temporary issues, and shifts of focus in the topic chains. We applied our framework to nine months of Korean Web news corpus and present our findings.},
  eventtitle = {International {{Conference}} on {{Intelligent Text Processing}} and {{Computational Linguistics}}},
  isbn = {978-3-642-19436-8 978-3-642-19437-5},
  langid = {english}
}

@inproceedings{korencic_getting_2015,
  title = {Getting the {{Agenda Right}}: {{Measuring Media Agenda Using Topic Models}}},
  shorttitle = {Getting the {{Agenda Right}}},
  booktitle = {Proceedings of the 2015 {{Workshop}} on {{Topic Models}}: {{Post-Processing}} and {{Applications}}},
  author = {Korenčić, Damir and Ristov, Strahil and Šnajder, Jan},
  date = {2015},
  series = {{{TM}} '15},
  pages = {61--66},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2809936.2809942},
  url = {http://doi.acm.org/10.1145/2809936.2809942},
  abstract = {Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.},
  isbn = {978-1-4503-3784-7},
  keywords = {agenda measuring,agenda setting,document tagging,multilabel classification,news media analysis,topic modeling}
}

@article{larcinese_partisan_2011,
  title = {Partisan Bias in Economic News: {{Evidence}} on the Agenda-Setting Behavior of {{U}}.{{S}}. Newspapers},
  shorttitle = {Partisan Bias in Economic News},
  author = {Larcinese, Valentino and Puglisi, Riccardo and Snyder, James M.},
  date = {2011-10-01},
  journaltitle = {Journal of Public Economics},
  shortjournal = {Journal of Public Economics},
  series = {Special {{Issue}}: {{The Role}} of {{Firms}} in {{Tax Systems}}},
  volume = {95},
  number = {9},
  pages = {1178--1189},
  issn = {0047-2727},
  doi = {10.1016/j.jpubeco.2011.04.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0047272711000715},
  urldate = {2019-01-19},
  abstract = {We study the agenda-setting political behavior of a large sample of U.S. newspapers during the 1996–2005 period. Our purpose is to examine the intensity of coverage of economic issues as a function of the underlying economic conditions and the political affiliation of the incumbent president, focusing on unemployment, inflation, the federal budget and the trade deficit. We investigate whether there is any significant correlation between the endorsement policy of newspapers, and the differential coverage of bad/good economic news as a function of the president's political affiliation. We find evidence that newspapers with pro-Democratic endorsement pattern systematically give more coverage to high unemployment when the incumbent president is a Republican than when the president is Democratic, compared to newspapers with pro-Republican endorsement pattern. This result is robust to controlling for the partisanship of readers. We find similar but less robust results for the trade deficit. We also find some evidence that newspapers cater to the partisan tastes of readers in the coverage of the budget deficit. We find no evidence of a partisan bias – or at least of a bias that is correlated with the endorsement or reader partisanship – for stories on inflation.},
  keywords = {Information,Mass media,Media bias,News},
  file = {/Users/franzilow/Zotero/storage/T7Z3DYRR/Larcinese et al. - 2011 - Partisan bias in economic news Evidence on the ag.pdf;/Users/franzilow/Zotero/storage/EVVMYWAY/S0047272711000715.html}
}

@book{larsson_rightwingers_2019,
  title = {{{RIGHT-WINGERS ON THE RISE ONLINE}} - {{INSIGHTS FROM THE}} 2018 {{SWEDISH ELECTIONS}}},
  author = {Larsson, Anders},
  date = {2019-10-18},
  doi = {10.13140/RG.2.2.29315.66086},
  abstract = {Given the recent rise of hyperpartisan media-often described as purveyors of 'fake news'-and populist right-wing parties across a series of western contexts, the present study details the degree to which these actors succeed in overtaking their more mainstream competitors when it comes to audience engagement on Facebook. Focusing on the one-month period leading up to the 2018 Swedish national elections, the study finds that right-wing actors across the media and the political sector are more successful in engaging their Facebook followers than their competitors, often utilizing sensational rhetoric and hate-mongering as campaign techniques. As audience engagement is a key factor for social media success, the study closes by providing a discussion on the repercussions for professionals within the media and the political sector.}
}

@report{law_constitutional_2016,
  type = {SSRN Scholarly Paper},
  title = {Constitutional {{Archetypes}}},
  author = {Law, David S.},
  date = {2016-12-05},
  number = {ID 2732519},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=2732519},
  urldate = {2017-11-09},
  abstract = {It is a core function of constitutions to justify the existence and organization of the state. The ideological narratives embedded in constitutions are not fundamentally unique, however, but instead derive from a limited number of competing models. Each model is defined by a particular type of justification for the existence and organization of the state, and by a symbiotic relationship with a particular legal tradition. These models are so ubiquitous and elemental that they amount to constitutional archetypes.This Article contends as an empirical matter that constitutional narratives of the state boil down to a combination of three basic archetypes–namely, a liberal archetype, a statist archetype, and a universalist archetype. The liberal archetype is closely identified with the common law tradition and views the state as a potentially oppressive concentration of authority in need of regulation and restraint. In keeping with this conception of the state, liberal constitutions emphasize the imposition of limits upon government in the form of negative and procedural rights, as well as a strong and independent judiciary to make these limits effective. The legitimacy of the state is contingent upon adherence to constitutional limits. Constitutions in this vein are largely agnostic as to what goals, if any, society as a whole should pursue through the mechanism of the state.The statist archetype, in contrast, is associated with the civil law tradition and hails the state as the embodiment of a distinctive community and the vehicle for the achievement of the community’s goals. The legitimacy of the state rests upon the strength of the state’s claim to represent the will of a community. Consequently, constitutions in this vein are attentive to the identity, membership, and symbols of the state. Other characteristics of a statist constitution include an emphasis on the articulation of collective goals and positive rights that contemplate an active role for the state, and an obligation on the part of citizens to cooperate with the state in the pursuit of shared goals.The universalist archetype, the newest and most prevalent of the three, is symbiotically intertwined with a post-World War II, post-Westphalian paradigm of international law that rests the legitimacy of the state upon the normative force of a global legal order that encompasses both constitutional law and international law. Characteristics of this archetype include explicit commitment to supranational institutions and supranational law and reliance on generic terms and concepts that can be found not only in a variety of national constitutions, but also in international legal instruments.Empirical evidence of the prevalence and content of these three basic archetypes can be found in the unlikeliest of places – namely, constitutional preambles. Preambles enjoy a reputation for expressing uniquely national values, identities, and narratives. If there is any part of a constitution that ought not to be reducible to a handful of recurring patterns, it is surely the preamble. Yet analysis of the world’s constitutional preambles using methods from computational linguistics suggests that they consist of a combination of the three archetypes. Estimation of a structural topic model yields a quantitative measure of the extent to which each preamble draws upon each archetype.The empirical analysis also highlights the growing commingling and interdependence of constitutional law and international law. The semantic patterns that characterize universalist preambles mirrors those found in leading international human rights instruments. The adoption of the same conceptual and normative vocabulary by both universalist constitutions and key international legal instruments signals the emergence of a globalized ideological dialect common to both domestic constitutional law and public international law. The rising use of this common language by constitutional drafters since World War II is a quantitative indicator of the growing extent to which constitutional law and public international law influence each other.},
  keywords = {archetype,automated,civil law,common law,constitution,constitution-making,constitution-writing,constitutional drafting,constitutional law,content analysis,empirical,ideology,international law,legal traditions,liberalism,preamble,statism,text analysis,topic model,universalism},
  file = {/Users/franzilow/Zotero/storage/IMHWX425/papers.html}
}

@article{lee_regression_2010,
  title = {Regression {{Discontinuity Designs}} in {{Economics}}},
  author = {Lee, David S. and Lemieux, Thomas},
  date = {2010-06},
  journaltitle = {Journal of Economic Literature},
  volume = {48},
  number = {2},
  pages = {281--355},
  issn = {0022-0515},
  doi = {10.1257/jel.48.2.281},
  url = {https://www.aeaweb.org/articles?id=10.1257/jel.48.2.281},
  urldate = {2021-06-20},
  abstract = {This paper provides an introduction and "user guide" to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a "quasi-experimental" design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD. ( JEL C21, C31)},
  langid = {english},
  keywords = {Quantile Regressions,Quantile Regressions; Multiple or Simultaneous Equation Models: Cross-Sectional Models,Single Equation Models,Single Variables: Cross-Sectional Models,Spatial Models,Treatment Effect Models},
  file = {/Users/franzilow/Zotero/storage/NPGEKQLU/Lee und Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf}
}

@article{lengauer_candidate_2013,
  title = {Candidate and Party Bias in the News and Its Effects on Party Choice: {{Evidence}} from {{Austria}}},
  shorttitle = {Candidate and Party Bias in the News and Its Effects on Party Choice},
  author = {Lengauer, Günther and Johann, David},
  date = {2013-01-01},
  journaltitle = {Studies in Communication Sciences},
  shortjournal = {Studies in Communication Sciences},
  volume = {13},
  number = {1},
  pages = {41--49},
  issn = {1424-4896},
  doi = {10.1016/j.scoms.2013.04.011},
  url = {http://www.sciencedirect.com/science/article/pii/S1424489613000143},
  urldate = {2018-10-20},
  abstract = {This paper investigates the impact of exposure to actor-related news bias on party choice in the 2008 Austrian elections. More specifically, this work introduces relative and relational measures of party and candidate biases in electoral reporting and outlines their effects on vote choice. The applied integrative understanding of party and candidate bias in the news, combining visibility and valence, results in a single measure classifying parties and party leaders in media-outlet-specific bias spectra. To measure bias exposure effects on party choice, the candidate and party bias levels in each newspaper are weighted with the individual exposure to these papers. This study rests on the analytical linkage between a media content analysis of Austrian daily newspapers and a representative post-election survey. Firstly, this investigation shows that the Austrian newspaper coverage is characterized by clear, not uniform but rather outlet-specific, biases toward parties and their leaders. Above all, the tabloid press primarily focuses on a strong polarization and duelization regarding the parties and the leaders contesting for chancellorship. Secondly, in a case study investigating the effects of news bias exposure on party choice regarding the governing parties, we find a positive relation between high exposure to advantageous party and candidate biases in newspapers and the probability to vote for the respective party.},
  keywords = {Content analysis,Election news,Media bias,Media coverage,Media effects,Party choice,Voting behavior},
  file = {/Users/franzilow/Zotero/storage/C3LEBPI7/S1424489613000143.html}
}

@article{lenz_looking_2011,
  title = {Looking the {{Part}}: {{Television Leads Less Informed Citizens}} to {{Vote Based}} on {{Candidates}}’ {{Appearance}}},
  shorttitle = {Looking the {{Part}}},
  author = {Lenz, Gabriel S. and Lawson, Chappell},
  date = {2011-07-01},
  journaltitle = {American Journal of Political Science},
  volume = {55},
  number = {3},
  pages = {574--589},
  issn = {1540-5907},
  doi = {10.1111/j.1540-5907.2011.00511.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2011.00511.x},
  urldate = {2018-10-20},
  abstract = {As long as there has been democratic government, skeptics have worried that citizens would base their choices and their votes on superficial considerations. A series of recent studies seems to validate these fears, suggesting that candidates who merely look more capable or attractive perform better in elections. In this article, we examine the underlying process behind the appearance effect. Specifically, we test whether the effect of appearance is more pronounced among those who know little about politics but are exposed to visual images of candidates. To do so, we combine appearance-based assessments of U.S. Senate and gubernatorial candidates with individual-level survey data measuring vote intent, political knowledge, and television exposure. Confirming long-standing concerns about image and television, we find that appealing-looking politicians benefit disproportionately from television exposure, primarily among less knowledgeable individuals.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/JT5RJNTV/Lenz und Lawson - 2011 - Looking the Part Television Leads Less Informed C.pdf;/Users/franzilow/Zotero/storage/EHR2VTFT/j.1540-5907.2011.00511.html}
}

@article{lott_newspaper_2014,
  title = {Is Newspaper Coverage of Economic Events Politically Biased?},
  author = {Lott, John R. and Hassett, Kevin A.},
  date = {2014-07-01},
  journaltitle = {Public Choice},
  shortjournal = {Public Choice},
  volume = {160},
  number = {1},
  pages = {65--108},
  issn = {1573-7101},
  doi = {10.1007/s11127-014-0171-5},
  url = {https://doi.org/10.1007/s11127-014-0171-5},
  urldate = {2019-01-07},
  abstract = {This paper develops an econometric technique to test for political bias in news reports that controls for the underlying character of the news reported. Because of the changing availability of the number of newspapers in Nexis/Lexis, two sets of time are examined: from January 1991 to May 2004 and from January 1985 to May 2004. Our results suggest that American newspapers tend to give more positive coverage to the same economic news when Democrats are in the White House than when Republicans are; a similar though smaller effect is found for Democratic control of Congress. Our results reject the claim that “reader diversity is a powerful force toward accuracy.” When all types of news are pooled into a single analysis, our results are significant. However, the results vary greatly depending upon which types of economic data are being reported. When newspapers are examined individually the only support that Republicans appear to obtain is from the president’s home state newspapers during his term. This is true for the Houston Chronicle under both Bushes and the Los Angeles Times during the Reagan administration. Contrary to rational expectations, media coverage affects people’s perceptions of the economy.},
  langid = {english},
  keywords = {Campaigns,Media bias,Voting},
  file = {/Users/franzilow/Zotero/storage/EDJFMSP7/Lott and Hassett - 2014 - Is newspaper coverage of economic events political.pdf}
}

@article{loughran_when_2011,
  title = {When {{Is}} a {{Liability Not}} a {{Liability}}? {{Textual Analysis}}, {{Dictionaries}}, and 10-{{Ks}}},
  shorttitle = {When {{Is}} a {{Liability Not}} a {{Liability}}?},
  author = {Loughran, Tim and Mcdonald, Bill},
  date = {2011-02-01},
  journaltitle = {The Journal of Finance},
  volume = {66},
  number = {1},
  pages = {35--65},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.2010.01625.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x/abstract},
  urldate = {2018-01-11},
  abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10-K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/ZFIUBAP9/Loughran und Mcdonald - 2011 - When Is a Liability Not a Liability Textual Analy.pdf;/Users/franzilow/Zotero/storage/TE73EQLZ/abstract.html}
}

@article{lucas_computer_2015,
  title = {Computer Assisted Text Analysis for Comparative Politics},
  author = {Lucas, Christopher and Nielsen, Richard and Roberts, Margaret and Stewart, Brandon and Storer, Alex and Tingley, Dustin},
  date = {2015},
  journaltitle = {Political Analysis},
  volume = {23},
  number = {2},
  pages = {254--277},
  file = {/Users/franzilow/Zotero/storage/N5EV4K52/computer-assisted-text-analysis-comparative-politics.html}
}

@article{luoma_development_2010,
  title = {The Development and Psychometric Properties of a New Measure of Perceived Stigma toward Substance Users},
  author = {Luoma, Jason B. and O'Hair, Alyssa K. and Kohlenberg, Barbara S. and Hayes, Steven C. and Fletcher, Lindsay},
  date = {2010},
  journaltitle = {Substance Use \& Misuse},
  shortjournal = {Subst Use Misuse},
  volume = {45},
  number = {1-2},
  eprint = {20025438},
  eprinttype = {pmid},
  pages = {47--57},
  issn = {1532-2491},
  doi = {10.3109/10826080902864712},
  abstract = {A self-report measure of perceived stigma toward substance users was developed and studied. An initial measure was created based on a previously developed scale that was rated by experts for content validity and quality of items. The scale, along with other measures, was administered to 252 people in treatment for substance problems in the United States during 2006-2007. Refinement efforts resulted in an eight-item scale with good face validity, construct validity, and adequate levels of internal consistency. Most relationships with other constructs were as expected. Findings suggest that perceived stigma is distinct from other forms of stigma.},
  langid = {english},
  pmcid = {PMC5067154},
  keywords = {Adolescent,Adult,Drug Users,Factor Analysis; Statistical,Female,Humans,Male,Middle Aged,Psychometrics,Reproducibility of Results,Social Perception,Stereotyping,Substance-Related Disorders}
}

@article{mazzoleni_media_2003,
  title = {The Media and Neo-Populism : A Contemporary Comparative Analysis},
  shorttitle = {The Media and Neo-Populism},
  author = {Mazzoleni, G. and Stewart, J. and Horsfield, Bruce},
  date = {2003},
  journaltitle = {undefined},
  url = {https://www.semanticscholar.org/paper/The-media-and-neo-populism-%3A-a-contemporary-Mazzoleni-Stewart/53e24419dbaddd6dbd4ca93aef60c472912fa9b0},
  urldate = {2022-01-03},
  abstract = {Tables and Figures Acknowledgments Foreword The Media and the Growth of Neo-Populism in Contemporary Democracies by Gianpietro Mazzoleni Striking a Responsive Chord: Mass Media and Right Wing Populism in Austria by Fritz Plasser and Peter A. Ulram The Media and Neo-populism in France by Guy Birenbaum and Marina Villa The Northern League and the Italian Media System by Roberto Biorcio The Bharatiya Janata Party, Ayodhya, and the Rise of Populist Politics in India by John McGuire and Geoffrey Reeves One Nation and the Australian Media by Bruce Horsfield and Julianne Stewart More Bad News: News Values and the Uneasy Relationship between the Reform Party and the Media in Canada by Richard W. Jenkins Ross Perot\&\#39;s Outsider Challenge: New and Old Media in American Presidential Campaigns by Jonathan Laurence Media Populism: Neo-populism in Latin America by Silvio Waisbord Conclusion: Power to the Media Managers by Julianne Stewart, Gianpietro Mazzoleni, and Bruce Horsfield Index About the Editors and Contributors},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/Y63KJ9G4/53e24419dbaddd6dbd4ca93aef60c472912fa9b0.html}
}

@incollection{mazzoleni_populism_2008,
  title = {Populism and the {{Media}}},
  booktitle = {Twenty-{{First Century Populism}}: {{The Spectre}} of {{Western European Democracy}}},
  author = {Mazzoleni, Gianpietro},
  editor = {Albertazzi, Daniele and McDonnell, Duncan},
  date = {2008},
  pages = {49--64},
  publisher = {{Palgrave Macmillan UK}},
  location = {{London}},
  doi = {10.1057/9780230592100_4},
  url = {https://doi.org/10.1057/9780230592100_4},
  urldate = {2020-12-20},
  abstract = {The European political landscape of the last decade has been home to numerous political figures that have stood out by virtue of their personality and their voicing of popular discontent. These include the likes of Jean-Marie Le Pen, Jörg Haider, Christoph Blocher, Pim Fortuyn and Silvio Berlusconi, all of whom are among the more recent manifestations of the populist political climate affecting much of contemporary Europe, as discussed in the introduction to this volume.},
  isbn = {978-0-230-59210-0},
  langid = {english},
  keywords = {Communication Style,News Medium,Political Arena,Political Communication,Populist Movement}
}

@incollection{mcauliffe_supervised_2008,
  title = {Supervised {{Topic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Mcauliffe, Jon D. and Blei, David M.},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  date = {2008},
  pages = {121--128},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
  file = {/Users/franzilow/Zotero/storage/AKKVXDQ9/Mcauliffe und Blei - 2008 - Supervised Topic Models.pdf;/Users/franzilow/Zotero/storage/KQV8JHW8/3328-supervised-topic-models.html}
}

@article{mccombs_agendasetting_1972,
  title = {The {{Agenda-Setting Function}} of {{Mass Media}}},
  author = {McCombs, Maxwell E. and Shaw, Donald L.},
  date = {1972},
  journaltitle = {The Public Opinion Quarterly},
  volume = {36},
  number = {2},
  pages = {176--187},
  issn = {0033-362X},
  url = {https://www.jstor.org/stable/2747787},
  urldate = {2018-08-14},
  abstract = {In choosing and displaying news, editors, newsroom staff, and broadcasters play an important part in shaping political reality. Readers learn not only about a given issue, but also how much importance to attach to that issue from the amount of information in a news story and its position. In reflecting what candidates are saying during a campaign, the mass media may well determine the impirtant issues-that is, the media may set the "agenda" of the compaign.}
}

@article{mccombs_look_2005,
  title = {A {{Look}} at {{Agenda-setting}}: Past, Present and Future},
  shorttitle = {A {{Look}} at {{Agenda-setting}}},
  author = {McCombs, Maxwell},
  date = {2005-11-01},
  journaltitle = {Journalism Studies},
  volume = {6},
  number = {4},
  pages = {543--557},
  issn = {1461-670X},
  doi = {10.1080/14616700500250438},
  url = {https://doi.org/10.1080/14616700500250438},
  urldate = {2018-10-20},
  file = {/Users/franzilow/Zotero/storage/ZHVLE86I/14616700500250438.html}
}

@inproceedings{mimno_bayesian_2011,
  title = {Bayesian {{Checking}} for {{Topic Models}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mimno, David and Blei, David},
  date = {2011},
  series = {{{EMNLP}} '11},
  pages = {227--237},
  publisher = {{Association for Computational Linguistics}},
  location = {{Stroudsburg, PA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
  abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
  isbn = {978-1-937284-11-4},
  file = {/Users/franzilow/Zotero/storage/MFUGSEJM/Mimno und Blei - 2011 - Bayesian Checking for Topic Models.pdf}
}

@inproceedings{mimno_optimizing_2011,
  title = {Optimizing {{Semantic Coherence}} in {{Topic Models}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
  date = {2011},
  series = {{{EMNLP}} '11},
  pages = {262--272},
  publisher = {{Association for Computational Linguistics}},
  location = {{Stroudsburg, PA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
  abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
  isbn = {978-1-937284-11-4},
  file = {/Users/franzilow/Zotero/storage/758M9D8V/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf}
}

@inproceedings{mimno_optimizing_2011a,
  title = {Optimizing {{Semantic Coherence}} in {{Topic Models}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
  date = {2011},
  series = {{{EMNLP}} '11},
  pages = {262--272},
  publisher = {{Association for Computational Linguistics}},
  location = {{Stroudsburg, PA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
  abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
  isbn = {978-1-937284-11-4},
  file = {/Users/franzilow/Zotero/storage/8ATB9444/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf}
}

@inproceedings{minka_expectationpropagation_2002,
  title = {Expectation-Propagation for the {{Generative Aspect Model}}},
  booktitle = {Proceedings of the {{Eighteenth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Minka, Thomas and Lafferty, John},
  date = {2002},
  series = {{{UAI}}'02},
  pages = {352--359},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=2073876.2073918},
  abstract = {The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets.},
  isbn = {978-1-55860-897-9}
}

@inproceedings{mishler_using_2015,
  title = {Using {{Structural Topic Modeling}} to {{Detect Events}} and {{Cluster Twitter Users}} in the {{Ukrainian Crisis}}},
  booktitle = {{{HCI International}} 2015 - {{Posters}}’ {{Extended Abstracts}}},
  author = {Mishler, Alan and Crabb, Erin Smith and Paletz, Susannah and Hefright, Brook and Golonka, Ewa},
  date = {2015-08-02},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {639--644},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-21380-4_108},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108},
  urldate = {2017-10-12},
  abstract = {Structural topic modeling (STM) is a recently introduced technique to model how the content of a collection of documents changes as a function of variables such as author identity or time of writing. We present two proof-of-concept applications of STM using Russian social media data. In our first study, we model how topics change over time, showing that STM can be used to detect significant events such as the downing of Malaysia Air Flight 17. In our second study, we model how topical content varies across a set of authors, showing that STM can be used to cluster Twitter users who are sympathetic to Ukraine versus Russia as well as to cluster accounts that are suspected to belong to the same individual (so-called “sockpuppets”). Structural topic modeling shows promise as a tool for analyzing social media data, a domain that has been largely ignored in the topic modeling literature.},
  eventtitle = {International {{Conference}} on {{Human-Computer Interaction}}},
  isbn = {978-3-319-21379-8 978-3-319-21380-4},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/ACUCPA8U/Mishler et al. - 2015 - Using Structural Topic Modeling to Detect Events a.pdf;/Users/franzilow/Zotero/storage/NG9CQ4T9/978-3-319-21380-4_108.html}
}

@report{mueller_reading_2016,
  type = {SSRN Scholarly Paper},
  title = {Reading between the {{Lines}}: {{Prediction}} of {{Political Violence Using Newspaper Text}}},
  shorttitle = {Reading between the {{Lines}}},
  author = {Mueller, Hannes Felix and Rauh, Christopher},
  date = {2016-09-01},
  number = {ID 2843535},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=2843535},
  urldate = {2017-11-09},
  abstract = {This article provides a new methodology to predict armed conflict by using newspaper text. Through machine learning, vast quantities of newspaper text are reduced to interpretable topics. We propose the use of the within-country variation of these topics to predict the timing of conflict. This allows us to avoid the tendency of predicting conflict only in countries where it occurred before. We show that the within-country variation of topics is an extremely robust predictor of conflict and becomes particularly useful when new conflict risks arise. Two aspects seem to be responsible for these features. Topics provide depth because they consist of changing, long lists of terms which makes them able to capture the changing context of conflict. At the same time topics provide width because they summarize all text, including coverage of stabilizing factors.},
  keywords = {Civil War,conflict,Forecasting,Latent Dirichlet Allocation.,Machine Learning,panel data,Topic Models},
  file = {/Users/franzilow/Zotero/storage/R7FGX7V9/papers.html}
}

@article{mullainathan_market_2005,
  title = {The {{Market}} for {{News}}},
  author = {Mullainathan, Sendhil and Shleifer, Andrei},
  date = {2005-09},
  journaltitle = {American Economic Review},
  volume = {95},
  number = {4},
  pages = {1031--1053},
  issn = {0002-8282},
  doi = {10.1257/0002828054825619},
  url = {https://www.aeaweb.org/articles?id=10.1257/0002828054825619},
  urldate = {2021-11-07},
  abstract = {We investigate the market for news under two assumptions: that readers hold beliefs which they like to see confirmed, and that newspapers can slant stories toward these beliefs. We show that, on the topics where readers share common beliefs, one should not expect accuracy even from competitive media: competition results in lower prices, but common slanting toward reader biases. On topics where reader beliefs diverge (such as politically divisive issues), however, newspapers segment the market and slant toward extreme positions. Yet in the aggregate, a reader with access to all news sources could get an unbiased perspective. Generally speaking, reader heterogeneity is more important for accuracy in media than competition per se.},
  langid = {english},
  keywords = {Belief; Entertainment,Communication,Information and Knowledge,Learning,Media,Search},
  file = {/Users/franzilow/Zotero/storage/BPH9AZPW/Mullainathan und Shleifer - 2005 - The Market for News.pdf;/Users/franzilow/Zotero/storage/6RE4U7MI/articles.html}
}

@article{muller_polarizing_2017,
  title = {The {{Polarizing Impact}} of {{News Coverage}} on {{Populist Attitudes}} in the {{Public}}: {{Evidence From}} a {{Panel Study}} in {{Four European Democracies}}},
  shorttitle = {The {{Polarizing Impact}} of {{News Coverage}} on {{Populist Attitudes}} in the {{Public}}},
  author = {Müller, Philipp and Schemer, Christian and Wettstein, Martin and Schulz, Anne and Wirz, Dominique and Engesser, Sven and Wirth, Werner},
  date = {2017-12-01},
  journaltitle = {Journal of Communication},
  shortjournal = {Journal of Communication},
  volume = {67},
  doi = {10.1111/jcom.12337},
  abstract = {The study explores how news messages carrying parts of the populist ideology contribute to a polarization of public opinion about populism. It combines a content analysis of news coverage on two policy areas (N = 7,119 stories) with a two-wave panel survey (N = 2,338) in four European metropolitan regions (Berlin, Paris, London, Zurich). In three regions, unopposed media messages with a populist stance have a conditional effect on populist attitudes that depends on prior convictions. A higher dose of exposure to populist news coverage enhances both prior agreement and disagreement with populism. While the observed interaction patterns vary between regions, the general picture suggests that populist messages in the news foster polarization between public support and disapproval of populism.}
}

@incollection{naskar_sentiment_2016,
  title = {Sentiment {{Analysis}} in {{Social Networks}} through {{Topic}} Modeling},
  booktitle = {{{LREC}}},
  author = {Naskar, Debashis and Mokaddem, Sidahmed and Rebollo, Miguel and Onaindia, Eva},
  date = {2016},
  url = {/paper/Sentiment-Analysis-in-Social-Networks-through-Topic-Naskar-Mokaddem/00052e8713adf52290c260208cafec05f70035d6},
  urldate = {2018-08-13},
  abstract = {In this paper, we analyze the sentiments derived from the conversations that occur in social networks. Our goal is to identify the sentiments of the users in the social network through their conversations. We conduct a study to determine whether users of social networks (twitter in particular) tend to gather together according to the likeness of their sentiments. In our proposed framework, (1) we use ANEW, a lexical dictionary to identify affective emotional feelings associated to a message according to the Russell’s model of affection; (2) we design a topic modeling mechanism called Sent LDA, based on the Latent Dirichlet Allocation (LDA) generative model, which allows us to find the topic distribution in a general conversation and we associate topics with emotions; (3) we detect communities in the network according to the density and frequency of the messages among the users; and (4) we compare the sentiments of the communities by using the Russell’s model of affect versus polarity and we measure the extent to which topic distribution strengthen likeness in the sentiments of the users of a community. This works contributes with a topic modeling methodology to analyze the sentiments in conversations that take place in social networks.},
  file = {/Users/franzilow/Zotero/storage/WKUIPZ8D/Naskar et al. - 2016 - Sentiment Analysis in Social Networks through Topi.pdf;/Users/franzilow/Zotero/storage/HDI4WHSA/00052e8713adf52290c260208cafec05f70035d6.html}
}

@inproceedings{newman_automatic_2010,
  title = {Automatic {{Evaluation}} of {{Topic Coherence}}},
  booktitle = {Human {{Language Technologies}}: {{The}} 2010 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
  date = {2010},
  series = {{{HLT}} '10},
  pages = {100--108},
  publisher = {{Association for Computational Linguistics}},
  location = {{Stroudsburg, PA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=1857999.1858011},
  abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
  isbn = {978-1-932432-65-7},
  file = {/Users/franzilow/Zotero/storage/QJIP655P/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf}
}

@article{newman_distributed_2009,
  title = {Distributed {{Algorithms}} for {{Topic Models}}},
  author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
  date = {2009-12},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {1801--1828},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=1577069.1755845},
  urldate = {2018-01-23},
  abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
  file = {/Users/franzilow/Zotero/storage/8WPWT44A/Newman et al. - 2009 - Distributed Algorithms for Topic Models.pdf}
}

@report{newman_reuters_2018,
  title = {Reuters {{Institute Digital News Report}} 2018},
  author = {Newman, Nic and Fletcher, Richard and Kalogeropoulos, Antonis and Levy, David and Kleis Nielsen, Rasmus},
  date = {2018},
  institution = {{Reuters Institute for the Study of Journalism}},
  url = {http://media.digitalnewsreport.org/wp-content/uploads/2018/06/digital-news-report-2018.pdf?x89475}
}

@article{ngai_review_2016,
  title = {A {{REVIEW OF THE LITERATURE ON APPLICATIONS OF TEXT MINING IN POLICY MAKING}}},
  author = {Ngai, E. W. T. and Lee, P. T. Y.},
  date = {2016-06-26},
  journaltitle = {PACIS 2016 Proceedings},
  url = {https://aisel.aisnet.org/pacis2016/343},
  file = {/Users/franzilow/Zotero/storage/WA8JR4XD/343.html}
}

@article{nyman_news_2018,
  title = {News and Narratives in Financial Systems: Exploiting Big Data for Systemic Risk Assessment | {{Bank}} of {{England}}},
  author = {Nyman, Rickard and Kapadia, Sujit and Tuckett, David and Gregory, David and Ormerod, Paul and Smith, Robert},
  date = {2018-05-01},
  journaltitle = {Bank of England Working Paper},
  volume = {704},
  url = {https://www.bankofengland.co.uk/working-paper/2018/news-and-narratives-in-financial-systems},
  urldate = {2018-02-21},
  file = {/Users/franzilow/Zotero/storage/BK83G72M/news-and-narratives-in-financial-systems.html}
}

@article{oegema_personalization_2009,
  title = {Personalization in Political {{Television News}}: {{A}} 13-{{Wave Survey Study}} to {{Assess Effects}} of {{Text}} and {{Footage}}},
  shorttitle = {Personalization in Political {{Television News}}},
  author = {Oegema, Dirk and Kleinnijenhuis, Jan},
  date = {2009},
  journaltitle = {Communications},
  volume = {25},
  number = {1},
  pages = {43--60},
  issn = {1613-4087},
  doi = {10.1515/comm.2000.25.1.43},
  url = {https://www.degruyter.com/view/j/comm.2000.25.issue-1/comm.2000.25.1.43/comm.2000.25.1.43.xml},
  urldate = {2019-04-19}
}

@article{oliveira_can_2017,
  title = {Can Social Media Reveal the Preferences of Voters? {{A}} Comparison between Sentiment Analysis and Traditional Opinion Polls},
  shorttitle = {Can Social Media Reveal the Preferences of Voters?},
  author = {Oliveira, Daniel José Silva and Bermejo, Paulo Henrique de Souza and family=Santos, given=Pâmela Aparecida, prefix=dos, useprefix=false},
  date = {2017-01-02},
  journaltitle = {Journal of Information Technology \& Politics},
  volume = {14},
  number = {1},
  pages = {34--45},
  issn = {1933-1681},
  doi = {10.1080/19331681.2016.1214094},
  url = {https://doi.org/10.1080/19331681.2016.1214094},
  urldate = {2018-08-15},
  abstract = {This study aims to determine whether the results obtained by applying sentiment analysis to data extracted from social media can reveal the political preferences of citizens to a greater degree of accuracy than traditional public opinion surveys. The researcher collected and analyzed 92,441 tweets related to the Brazilian presidential candidates during the second round of elections in 2014. The analysis results were compared with six voter preference polls conducted by the Datafolha Research Institute. The results showed that sentiment analysis may indicate voter preferences at an accuracy that was only 1\% to 8\% different than that of traditional research, which has an average accuracy of 81.05\%.},
  keywords = {Elections,opinion mining,opinion polls,sentiment analysis,social media,Twitter},
  file = {/Users/franzilow/Zotero/storage/VE3AT736/19331681.2016.html}
}

@article{padmaja_evaluating_2014,
  title = {Evaluating {{Sentiment Analysis Methods}} and {{Identifying Scope}} of {{Negation}} in {{Newspaper Articles}}},
  author = {Padmaja, S. and Fatima, Prof S. Sameen and Bandu, Sasidhar},
  date = {2014},
  journaltitle = {International Journal of Advanced Research in Artificial Intelligence (IJARAI)},
  volume = {3},
  number = {11},
  doi = {10.14569/IJARAI.2014.031101},
  url = {http://thesai.org/Publications/ViewPaper?Volume=3&Issue=11&Code=IJARAI&SerialNo=1},
  urldate = {2018-03-19},
  abstract = {Automatic detection of linguistic negation in free text is a demanding need for many text processing applications including Sentiment Analysis. Our system uses online news archives from two different resources namely NDTV and The Hindu. While dealing with news articles, we performed three subtasks namely identifying the target; separation of good and bad news content from the good and bad sentiment expressed on the target and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. In this paper, our main focus was on evaluating and comparing three sentiment analysis methods (two machine learning based and one lexical based) and also identifying the scope of negation in news articles for two political parties namely BJP and UPA by using three existing methodologies. They were Rest of the Sentence (RoS), Fixed Window Length (FWL) and Dependency Analysis (DA). Among the sentiment methods the best F-measure was SVM with the values 0.688 and 0.657 for BJP and UPA respectively. On the other hand, the F measures for RoS, FWL and DA were 0.58, 0.69 and 0.75 respectively. We observed that DA was performing better than the other two. Among 1675 sentences in the corpus, according to annotator I, 1,137 were positive and 538 were negative whereas according to annotator II, 1,130 were positive and 545 were negative. Further we also identified the score of each sentence and calculated the accuracy on the basis of average score of both the annotators.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/3NRWA2F2/Padmaja et al. - 2014 - Evaluating Sentiment Analysis Methods and Identify.pdf;/Users/franzilow/Zotero/storage/RY3NFDRX/ViewPaper.html}
}

@thesis{paul_crosscollection_2009,
  type = {mathesis},
  title = {Cross-{{Collection Topic Models}}: {{Automatically Comparing}} and {{Contrasting Text}}},
  shorttitle = {Cross-{{Collection Topic Models}}},
  author = {Paul, Michael},
  date = {2009},
  institution = {{University of Illinois at Urbana-Champaign}},
  abstract = {This paper describes cross-collection latent Dirichlet allocation (ccLDA), a probabilistic topic model that captures meaningful word co-occurrences across multiple text collections. The model is applied to three different applications: discovering cultural differences in blogs and forums from different countries, discovering research topics across multiple scientific disciplines, and comparing editorial differences between multiple media sources. A variety of qualitative and quantitative evaluations of ccLDA are performed, including log-likelihood measurements and performance measurements of the model used as a generative classifier. Improvements over previous work are demonstrated. Finally, possible extensions and modifications to the model are presented with promising results. 1},
  file = {/Users/franzilow/Zotero/storage/H2I347RL/Paul - Cross-Collection Topic Models Automatically Compa.pdf;/Users/franzilow/Zotero/storage/KGFW8BSS/summary.html}
}

@article{paul_qualitative_2017,
  title = {Qualitative and Quantitative Central Bank Communication and Inflation Expectations},
  author = {Paul, Hubert},
  date = {2017},
  journaltitle = {The B.E. Journal of Macroeconomics},
  volume = {17},
  number = {1},
  pages = {1--41},
  url = {https://ideas.repec.org/a/bpj/bejmac/v17y2017i1p41n7.html},
  urldate = {2018-03-07},
  abstract = {We aim to investigate the simultaneous and interacted effects of central bank qualitative and quantitative communication on private inflation expectations, measured with survey and market-based measures. The effects of ECB inflation projections and Governing Council members’ speeches are identified through an instrumental-variables estimation using a principal component analysis to generate relevant instruments. We find that ECB projections have a positive effect on current-year forecasts, and that ECB projections and speeches are substitutes at longer horizons. Moreover, ECB speeches and the ECB rate reinforce the effect of ECB projections when they are consistent, and convey the same signal about inflationary pressures.},
  langid = {english},
  keywords = {central bank communication,European central bank,monetary policy,principal component analysis},
  file = {/Users/franzilow/Zotero/storage/55VNARXK/v17y2017i1p41n7.html}
}

@inproceedings{phan_learning_2008,
  title = {Learning to {{Classify Short}} and {{Sparse Text}} \& {{Web}} with {{Hidden Topics}} from {{Large-Scale Data Collections}}},
  booktitle = {Proceedings of the 17th {{International World Wide Web Conference}} ({{WWW}} 2008)},
  author = {Phan, Xuan-Hieu and Nguyen, Le and Horiguchi, Susumu},
  date = {2008-01-01},
  pages = {91--100},
  location = {{Beijing, China}},
  doi = {10.1145/1367497.1367510},
  abstract = {This paper presents a general framework for building classi- fiers that deal with short and sparse text \& Web segments by making the most of hidden topics discovered from large- scale data collections. The main motivation of this work is that many classification tasks working with short segments of text \& Web, such as search snippets, forum \& chat mes- sages, blog \& news feeds, product reviews, and book \& movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data bet- ter. The underlying idea of the framework is that for each classification task, we collect a large-scale external data col- lection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.},
  file = {/Users/franzilow/Zotero/storage/SP33XWPF/221023426_Learning_to_Classify_Short_and_Sparse_Text_Web_with_Hidden_Topics_from_Large-Scale_Dat.pdf}
}

@thesis{ponweiser_latent_2012,
  type = {Theses / Institute for Statistics and Mathematics},
  title = {Latent {{Dirichlet Allocation}} in {{R}}},
  author = {Ponweiser, Martin},
  date = {2012-05},
  institution = {{WU Vienna University of Economics and Business}},
  url = {http://epub.wu.ac.at/3558/},
  urldate = {2017-10-13},
  abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation (LDA), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes. This thesis focuses on LDA's practical application. Its main goal is the replication of the data analyses from the 2004 LDA paper ``Finding scientific topics'' by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R\textasciitilde package topicmodels by Bettina Grün and Kurt Hornik. The complete process, including extraction of a text corpus from the PNAS journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with LDA. (author's abstract)},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/AKASJPTP/Ponweiser - 2012 - Latent Dirichlet Allocation in R.pdf;/Users/franzilow/Zotero/storage/E5J47GZP/3558.html}
}

@incollection{prat_political_2013,
  title = {The {{Political Economy}} of {{Mass Media}}},
  booktitle = {Advances in {{Economics}} and {{Econometrics}}: {{Tenth World Congress}}: {{Volume}} 2: {{Applied Economics}}},
  author = {Prat, Andrea and Strömberg, David},
  editor = {Acemoglu, Daron and Dekel, Eddie and Arellano, Manuel},
  date = {2013},
  series = {Econometric {{Society Monographs}}},
  volume = {2},
  pages = {135--187},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9781139060028.004},
  url = {https://www.cambridge.org/core/books/advances-in-economics-and-econometrics/political-economy-of-mass-media/C94C8E86F1DEFF413C83494CB1D92ADD},
  urldate = {2022-01-09},
  abstract = {IntroductionIn the last decade, a sizable number of economists have begun to study the behavior and political effects of mass media. In this survey, we propose a way to organize this body of research, we attempt to summarize the key insights that have been learned so far, and we suggest potentially important open questions.We structure the discussion in sections covering background, transparency, capture, informative coverage, and ideological bias. Section 2.0 begins with an overview of how economics and other disciplines approach this field and defines the scope of this survey. Section 3.0, discusses the benefits and costs of transparency in politics: Under which situations do voters benefit from receiving more information?Section 4.0 addresses under which conditions the government will prevent the media from performing its information-provision task. Media capture is a present or latent risk in most developing and many developed countries. We present a theory of endogenous capture and survey the growing empirical literature on the extent and determinants of capture. As demonstrated herein, different sources of evidence provide support for the idea that ownership plurality is the most effective defense against capture.Section 5.0 discusses a crucial theme in media studies – namely, how informative media coverage affects political accountability and government policy. A model of policy choice with endogenous media coverage supplies an array of testable implications, used to organize the existing empirical work. The key questions are: What drives media coverage of politics? How does this coverage influence government policy, the actions and selection of politicians, and the information levels and voting behavior of the public?},
  isbn = {978-1-107-01605-7},
  file = {/Users/franzilow/Zotero/storage/TCBDTQ64/C94C8E86F1DEFF413C83494CB1D92ADD.html}
}

@article{prior_incumbent_2006,
  title = {The {{Incumbent}} in the {{Living Room}}: {{The Rise}} of {{Television}} and the {{Incumbency Advantage}} in {{U}}.{{S}}. {{House Elections}}},
  shorttitle = {The {{Incumbent}} in the {{Living Room}}},
  author = {Prior, Markus},
  date = {2006-08-01},
  journaltitle = {Journal of Politics},
  volume = {68},
  number = {3},
  pages = {657--673},
  issn = {1468-2508},
  doi = {10.1111/j.1468-2508.2006.00452.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2508.2006.00452.x},
  urldate = {2018-10-20},
  abstract = {This study shows that the growth of television contributed to the rise in the incumbency advantage in U.S. House elections during the 1960s. Incumbents received positive coverage throughout their term and were generally more newsworthy and better funded than their challengers during the campaign. Less-educated voters, for whom television presented a new, less demanding source of news, were most affected by local television. Analysis of National Elections Studies data reveals that less-educated respondents were more knowledgeable about the incumbent and more likely to vote for the incumbent in districts with television stations. Aggregate analysis shows that incumbents’ vote margins increased in proportion to the number of television stations in their districts.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/YW6ETPHF/Prior - 2006 - The Incumbent in the Living Room The Rise of Tele.pdf;/Users/franzilow/Zotero/storage/RKPHGYD2/j.1468-2508.2006.00452.html}
}

@article{pritchard_association_2000,
  title = {Association {{Mapping}} in {{Structured Populations}}},
  author = {Pritchard, Jonathan K. and Stephens, Matthew and Rosenberg, Noah A. and Donnelly, Peter},
  date = {2000-07},
  journaltitle = {American Journal of Human Genetics},
  shortjournal = {Am J Hum Genet},
  volume = {67},
  number = {1},
  eprint = {10827107},
  eprinttype = {pmid},
  pages = {170--181},
  issn = {0002-9297},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1287075/},
  urldate = {2018-01-19},
  abstract = {The use, in association studies, of the forthcoming dense genomewide collection of single-nucleotide polymorphisms (SNPs) has been heralded as a potential breakthrough in the study of the genetic basis of common complex disorders. A serious problem with association mapping is that population structure can lead to spurious associations between a candidate marker and a phenotype. One common solution has been to abandon case-control studies in favor of family-based tests of association, such as the transmission/disequilibrium test (TDT), but this comes at a considerable cost in the need to collect DNA from close relatives of affected individuals. In this article we describe a novel, statistically valid, method for case-control association studies in structured populations. Our method uses a set of unlinked genetic markers to infer details of population structure, and to estimate the ancestry of sampled individuals, before using this information to test for associations within subpopulations. It provides power comparable with the TDT in many settings and may substantially outperform it if there are conflicting associations in different subpopulations.},
  pmcid = {PMC1287075},
  file = {/Users/franzilow/Zotero/storage/5C5QX5GU/Pritchard et al. - 2000 - Association Mapping in Structured Populations.pdf}
}

@article{pritchard_inference_2000,
  title = {Inference of Population Structure Using Multilocus Genotype Data},
  author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
  date = {2000-06},
  journaltitle = {Genetics},
  shortjournal = {Genetics},
  volume = {155},
  number = {2},
  eprint = {10835412},
  eprinttype = {pmid},
  pages = {945--959},
  issn = {0016-6731},
  abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
  langid = {english},
  pmcid = {PMC1461096},
  keywords = {Algorithms,Cluster Analysis,Genetics; Population,Genotype,Humans,Models; Genetic}
}

@article{puglisi_being_2011,
  title = {Being {{The New York Times}}: The {{Political Behaviour}} of a {{Newspaper}}},
  shorttitle = {Being {{The New York Times}}},
  author = {Puglisi, Riccardo},
  date = {2011},
  journaltitle = {The B.E. Journal of Economic Analysis \& Policy},
  volume = {11},
  number = {1},
  pages = {1--34},
  url = {https://econpapers.repec.org/article/bpjbejeap/v_3a11_3ay_3a2011_3ai_3a1_3an_3a20.htm},
  urldate = {2019-01-19},
  abstract = {I analyse a dataset of news from The New York Times, from 1946 to 1997. Controlling for the activity of the incumbent president and the U.S. Congress across issues, I find that during a presidential campaign, The New York Times gives more emphasis to topics on which the Democratic party is perceived as more competent (civil rights, health care, labor and social welfare) when the incumbent president is a Republican. This is consistent with the hypothesis that The New York Times has a Democratic partisanship, with some “anti-incumbent” aspects, in that—during a presidential campaign—it gives more emphasis to issues over which the (Republican) incumbent is weak. To the extent that the interest of readers across issues is not systematically related with the political affiliation of the incumbent president and the election cycle, the observed changes in news coverage are consistent with The New York Times departing from demand-driven news coverage. In fact, I show that these findings are robust to controlling for Gallup data on the most important problem facing the country, which I use as a proxy for issue tastes of Times’ readers.},
  file = {/Users/franzilow/Zotero/storage/FAGD2JNZ/v_3a11_3ay_3a2011_3ai_3a1_3an_3a20.html}
}

@article{quinn_how_2010,
  title = {How to {{Analyze Political Attention}} with {{Minimal Assumptions}} and {{Costs}}},
  author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
  date = {2010-01-01},
  journaltitle = {American Journal of Political Science},
  volume = {54},
  number = {1},
  pages = {209--228},
  issn = {1540-5907},
  doi = {10.1111/j.1540-5907.2009.00427.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00427.x/abstract},
  abstract = {Previous methods of analyzing the substance of political attention have had to make several restrictive assumptions or been prohibitively costly when applied to large-scale political texts. Here, we describe a topic model for legislative speech, a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Our method estimates, rather than assumes, the substance of topics, the keywords that identify topics, and the hierarchical nesting of topics. We use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. Using a new database of over 118,000 speeches (70,000,000 words) from the Congressional Record, our model reveals speech topic categories that are both distinctive and meaningfully interrelated and a richer view of democratic agenda dynamics than had previously been possible.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/59JK6JEX/Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf;/Users/franzilow/Zotero/storage/QSVKCVD5/abstract.html}
}

@inproceedings{rabinovich_inverse_2014,
  title = {The {{Inverse Regression Topic Model}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 32},
  author = {Rabinovich, Maxim and Blei, David M.},
  date = {2014},
  series = {{{ICML}}'14},
  pages = {I-199--I-207},
  publisher = {{JMLR.org}},
  location = {{Beijing, China}},
  url = {http://dl.acm.org/citation.cfm?id=3044805.3044829},
  abstract = {Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.}
}

@book{ramage_characterizing_2010,
  title = {Characterizing {{Microblogs}} with {{Topic Models}}},
  author = {Ramage, Daniel and Dumais, Susan and Liebling, Daniel},
  date = {2010-01-01},
  journaltitle = {ICWSM},
  abstract = {As microblogging grows in popularity, services like Twitter are coming to support information gathering needs above and beyond their traditional roles as social networks. But most users' interaction with Twitter is still primarily focused on their social graphs, forcing the often inappropriate conflation of "people I follow" with "stuff I want to read." We characterize some information needs that the current Twitter interface fails to support, and argue for better representations of content for solving these challenges. We present a scalable implementation of a partially supervised learning model (Labeled LDA) that maps the content of the Twitter feed into dimensions. These dimensions correspond roughly to substance, style, status, and social characteristics of posts. We characterize users and tweets using this model, and present results on two information consumption oriented tasks.}
}

@book{ramage_labeled_2009,
  title = {Labeled {{LDA}}: {{A}} Supervised Topic Model for Credit Attribution in Multi-Labeled Corpora},
  shorttitle = {Labeled {{LDA}}},
  author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
  date = {2009},
  abstract = {A significant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. 1},
  file = {/Users/franzilow/Zotero/storage/WD2TZ58J/Ramage et al. - Labeled LDA A supervised topic model for credit a.pdf;/Users/franzilow/Zotero/storage/W5DGAHT5/summary.html}
}

@article{ravi_survey_2015,
  title = {A Survey on Opinion Mining and Sentiment Analysis: {{Tasks}}, Approaches and Applications},
  shorttitle = {A Survey on Opinion Mining and Sentiment Analysis},
  author = {Ravi, Kumar and Ravi, Vadlamani},
  date = {2015-11-01},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {89},
  pages = {14--46},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2015.06.015},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705115002336},
  urldate = {2018-10-11},
  abstract = {With the advent of Web 2.0, people became more eager to express and share their opinions on web regarding day-to-day activities and global issues as well. Evolution of social media has also contributed immensely to these activities, thereby providing us a transparent platform to share views across the world. These electronic Word of Mouth (eWOM) statements expressed on the web are much prevalent in business and service industry to enable customer to share his/her point of view. In the last one and half decades, research communities, academia, public and service industries are working rigorously on sentiment analysis, also known as, opinion mining, to extract and analyze public mood and views. In this regard, this paper presents a rigorous survey on sentiment analysis, which portrays views presented by over one hundred articles published in the last decade regarding necessary tasks, approaches, and applications of sentiment analysis. Several sub-tasks need to be performed for sentiment analysis which in turn can be accomplished using various approaches and techniques. This survey covering published literature during 2002–2015, is organized on the basis of sub-tasks to be performed, machine learning and natural language processing techniques used and applications of sentiment analysis. The paper also presents open issues and along with a summary table of a hundred and sixty-one articles.},
  keywords = {Lexica creation,Machine learning,Micro blog,Ontology,Opinion mining,Sentiment analysis,Social media},
  file = {/Users/franzilow/Zotero/storage/VYW3NR9I/S0950705115002336.html}
}

@article{rehs_structural_2020,
  title = {A Structural Topic Model Approach to Scientific Reorientation of Economics and Chemistry after {{German}} Reunification},
  author = {Rehs, Andreas},
  date = {2020-11-01},
  journaltitle = {Scientometrics},
  shortjournal = {Scientometrics},
  volume = {125},
  number = {2},
  pages = {1229--1251},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03640-0},
  url = {https://doi.org/10.1007/s11192-020-03640-0},
  urldate = {2020-11-28},
  abstract = {The detection of differences or similarities in large numbers of scientific publications is an open problem in scientometric research. In this paper we therefore develop and apply a machine learning approach based on structural topic modelling in combination with cosine similarity and a linear regression framework in order to identify differences in dissertation titles written at East and West German universities before and after German reunification. German reunification and its surrounding time period is used because it provides a structure with both minor and major differences in research topics that could be detected by our approach. Our dataset is based on dissertation titles in economics and business administration and chemistry from 1980 to 2010. We use university affiliation and year of the dissertation to train a structural topic model and then test the model on a set of unseen dissertation titles. Subsequently, we compare the resulting topic distribution of each title to every other title with cosine similarity. The cosine similarities and the regional and temporal origin of the dissertation titles they come from are then used in a linear regression approach. Our results on research topics in economics and business administration suggest substantial differences between East and West Germany before the reunification and a rapid conformation thereafter. In chemistry we observe minor differences between East and West before the reunification and a slightly increased similarity thereafter.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/NIELX3PB/Rehs - 2020 - A structural topic model approach to scientific re.pdf}
}

@article{reich_computerassisted_2014,
  title = {Computer-{{Assisted Reading}} and {{Discovery}} for {{Student Generated Text}} in {{Massive Open Online Courses}}},
  author = {Reich, Justin and Tingley, Dustin and Leder-Luis, Jetson and Roberts, Margaret E. and Stewart, Brandon},
  date = {2014-11-18},
  journaltitle = {Journal of Learning Analytics},
  volume = {2},
  number = {1},
  pages = {156--184},
  issn = {1929-7750},
  doi = {10.18608/jla.2015.21.8},
  url = {http://www.learning-analytics.info/journals/index.php/JLA/article/view/4138},
  urldate = {2017-11-09},
  abstract = {Dealing with the vast quantities of text that students generate in Massive Open Online Courses (MOOCs) and other large-scale online learning environments is a daunting challenge. Computational tools are needed to help instructional teams uncover themes and patterns as students write in forums, assignments, and surveys. This paper introduces to the learning analytics community the Structural Topic Model, an approach to language processing that can 1) find syntactic patterns with semantic meaning in unstructured text, 2) identify variation in those patterns across covariates, and 3) uncover archetypal texts that exemplify the documents within a topical pattern. We show examples of computationally aided discovery and reading in three MOOC settings: mapping students’ self-reported motivations, identifying themes in discussion forums, and uncovering patterns of feedback in course evaluations.},
  langid = {english},
  keywords = {computer‐assisted reading,Massive Open Online Courses,text analysis,topic modelling},
  file = {/Users/franzilow/Zotero/storage/BPWP7FIR/Reich et al. - 2014 - Computer-Assisted Reading and Discovery for Studen.pdf;/Users/franzilow/Zotero/storage/52JEF8DU/4138.html}
}

@article{roberts_model_2016,
  title = {A {{Model}} of {{Text}} for {{Experimentation}} in the {{Social Sciences}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
  date = {2016-07-02},
  journaltitle = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {988--1003},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1141684},
  url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
  abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
  keywords = {Causal inference,Experimentation,High-dimensional inference,Social sciences,text analysis,Variational approximation},
  file = {/Users/franzilow/Zotero/storage/ZMEDWA9E/01621459.2016.html}
}

@incollection{roberts_navigating_2016,
  title = {Navigating the {{Local Modes}} of {{Big Data}}: {{The Case}} of {{Topic Models}}.},
  booktitle = {Computational {{Social Science}}: {{Discovery}} and {{Prediction}}},
  author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
  date = {2016},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  file = {/Users/franzilow/Zotero/storage/TW4DVST8/navigating-local-modes-big-data-case-topic-models.html}
}

@article{roberts_stm_2016,
  title = {Stm: {{R Package}} for {{Structural Topic Models}}},
  shorttitle = {Stm},
  author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
  date = {2016-12-01},
  journaltitle = {Journal of Statistical Software},
  volume = {forthcoming},
  file = {/Users/franzilow/Zotero/storage/FAWQJE6X/stm-r-package-structural-topic-models.html}
}

@inproceedings{roberts_structural_2013,
  title = {The {{Structural Topic Model}} and {{Applied Social Science}}},
  booktitle = {Advances in {{Neural Information Processing Systems Workshop}} on {{Topic Models}}: {{Computation}}, {{Application}}, and {{Evaluation}}.},
  author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
  date = {2013},
  file = {/Users/franzilow/Zotero/storage/69G2KU2F/structural-topic-model-and-applied-social-science.html}
}

@article{roberts_structural_2014,
  title = {Structural {{Topic Models}} for {{Open-Ended Survey Responses}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  date = {2014-10-01},
  journaltitle = {American Journal of Political Science},
  shortjournal = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--1082},
  issn = {1540-5907},
  doi = {10.1111/ajps.12103},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12103/abstract},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/FD9RK878/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf;/Users/franzilow/Zotero/storage/6GGMT3JM/abstract.html}
}

@article{samuelson_aspects_1958,
  title = {Aspects of {{Public Expenditure Theories}}},
  author = {Samuelson, Paul A.},
  date = {1958},
  journaltitle = {The Review of Economics and Statistics},
  volume = {40},
  number = {4},
  pages = {332--338},
  issn = {0034-6535},
  doi = {10.2307/1926336},
  url = {http://www.jstor.org/stable/1926336}
}

@article{savigny_public_2002,
  title = {Public {{Opinion}}, {{Political Communication}} and the {{Internet}}},
  author = {Savigny, Heather},
  date = {2002-02-01},
  journaltitle = {Politics},
  volume = {22},
  number = {1},
  pages = {1--8},
  issn = {1467-9256},
  doi = {10.1111/1467-9256.00152},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9256.00152},
  urldate = {2018-08-14},
  abstract = {In contemporary society public opinion is generally mediated by the mass media, which has come to encompass the Habermasian ‘public sphere’. This arena is now characterised by the conflict between market and democratic principles, by competing interests of politicians and the media. The presentation of information for debate becomes distorted. The opinion of the ‘public’ is no longer created through deliberation, but is constructed through systems of communication, in conflict with political actors, who seek to retain control of the dissemination of information. The expansion of the internet as a new method of communication provides a potential challenge to the primacy of the traditional media and political parties as formers of public opinion.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/MFG6M57V/Savigny - 2002 - Public Opinion, Political Communication and the In.pdf;/Users/franzilow/Zotero/storage/55HY587S/1467-9256.html}
}

@article{schiller_development_2016,
  title = {Development of the {{Social Network Usage}} in {{Germany}} since 2012},
  author = {Schiller, Benjamin and Heimbach, Irina and Strufe, Thorsten and Hinz, Oliver},
  date = {2016},
  journaltitle = {Working Paper TU Darmstadt}
}

@article{shapiro_measuring_2017,
  title = {Measuring {{News Sentiment}}},
  author = {Shapiro, Adam and Sudhof, Moritz and Wilson, Daniel},
  date = {2017-01-05},
  journaltitle = {Federal Reserve Bank of San Francisco Working Paper},
  number = {2017-01},
  url = {https://econpapers.repec.org/paper/fipfedfwp/2017-01.htm},
  urldate = {2018-01-11},
  abstract = {We develop and assess new time series measures of economic sentiment based on computational text analysis of economic and financial newspaper articles from January 1980 to April 2015. The text analysis is based on predictive models estimated using machine learning techniques from Kanjoya. We analyze four alternative news sentiment indexes. We find that the news sentiment indexes correlate strongly with contemporaneous business cycle indicators. We also find that innovations to news sentiment predict future economic activity. Furthermore, in most cases, the news sentiment measures outperform the University of Michigan and Conference board measures in predicting the federal funds rate, consumption, employment, inflation, industrial production, and the S\&P500. For some of these economic outcomes, there is evidence that the news sentiment measures have significant predictive power even after conditioning on these survey-based measures.},
  file = {/Users/franzilow/Zotero/storage/TB95H8YV/Shapiro et al. - 2017 - Measuring News Sentiment.pdf;/Users/franzilow/Zotero/storage/CJ57433M/2017-01.html}
}

@article{sheets_media_2016,
  title = {Media {{Cues}} and {{Citizen Support}} for {{Right-Wing Populist Parties}}},
  author = {Sheets, Penelope and Bos, Linda and Boomgaarden, Hajo G.},
  date = {2016-09-01},
  journaltitle = {International Journal of Public Opinion Research},
  shortjournal = {Int J Public Opin Res},
  volume = {28},
  number = {3},
  pages = {307--330},
  publisher = {{Oxford Academic}},
  issn = {0954-2892},
  doi = {10.1093/ijpor/edv014},
  url = {https://academic.oup.com/ijpor/article/28/3/307/1750155},
  urldate = {2020-10-11},
  abstract = {Abstract.  Right-wing populist parties are thriving across Europe. Their success is usually attributed to demand-side voter factors and supply-side factors expl},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/Y33X8DAB/Sheets et al. - 2016 - Media Cues and Citizen Support for Right-Wing Popu.pdf;/Users/franzilow/Zotero/storage/MELQFU3B/1750155.html}
}

@article{silge_tidytext_2016,
  title = {Tidytext: {{Text Mining}} and {{Analysis Using Tidy Data Principles}} in {{R}}},
  shorttitle = {Tidytext},
  author = {Silge, Julia and Robinson, David},
  date = {2016-07-11},
  journaltitle = {The Journal of Open Source Software},
  doi = {10.21105/joss.00037},
  file = {/Users/franzilow/Zotero/storage/KBIXHDAH/305219559_tidytext_Text_Mining_and_Analysis_Using_Tidy_Data_Principles_in_R.pdf}
}

@article{simon_designing_1971,
  title = {Designing Organizations for an Information-Rich World},
  author = {Simon, Herbert A. and Deutsch, Karl W. and Shubik, Martin},
  date = {1971},
  journaltitle = {Computers, communications, and the public interest},
  series = {Computers, Communications, and the Public Interest. - {{Baltimore}}, {{Md}}. [u.a.] : {{Johns Hopkins Press}}, {{ISBN}} 0-8018-1135-{{X}}. - 1971, p. 37-72},
  file = {/Users/franzilow/Zotero/storage/NZ3D4I5D/10002817747.html}
}

@article{snyder_press_2010,
  title = {Press {{Coverage}} and {{Political Accountability}}},
  author = {Snyder, James~M. and Strömberg, David},
  date = {2010},
  journaltitle = {Journal of Political Economy},
  volume = {118},
  number = {2},
  pages = {355--408},
  issn = {0022-3808},
  doi = {10.1086/652903},
  url = {https://www.jstor.org/stable/10.1086/652903},
  urldate = {2018-08-22},
  abstract = {We estimate the impact of press coverage on citizen knowledge, politicians’ actions, and policy. We find that voters living in areas where, for exogenous reasons, the press covers their U.S. House representative less are less likely to recall their representative’s name and less able to describe and rate him or her. Congressmen who are less covered by the local press work less for their constituencies: they are less likely to stand witness before congressional hearings, to serve on constituency‐oriented committees (perhaps), and to vote against the party line. Finally, federal spending is lower in areas with exogenously lower press coverage of congressmen.},
  file = {/Users/franzilow/Zotero/storage/J8FIZV9G/Snyder und Strömberg - 2010 - Press Coverage and Political Accountability.pdf}
}

@incollection{socher_bayesian_2009,
  title = {A {{Bayesian Analysis}} of {{Dynamics}} in {{Free Recall}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler J. and Blei, David M.},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  date = {2009},
  pages = {1714--1722},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3720-a-bayesian-analysis-of-dynamics-in-free-recall.pdf},
  file = {/Users/franzilow/Zotero/storage/JQWKTPGH/Socher et al. - 2009 - A Bayesian Analysis of Dynamics in Free Recall.pdf;/Users/franzilow/Zotero/storage/ZFW3QA7C/3720-a-bayesian-analysis-of-dynamics-in-free-recall.html}
}

@unpublished{soelistio_simple_2015,
  title = {Simple {{Text Mining}} for {{Sentiment Analysis}} of {{Political Figure Using Naive Bayes Classifier Method}}},
  author = {Soelistio, Yustinus Eko and Surendra, Martinus Raditia Sigit},
  date = {2015-08-20},
  eprint = {1508.05163},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.12962/p9772338185001.a18},
  url = {http://arxiv.org/abs/1508.05163},
  urldate = {2018-10-11},
  abstract = {Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/franzilow/Zotero/storage/JQX3ZGVX/Soelistio und Surendra - 2015 - Simple Text Mining for Sentiment Analysis of Polit.pdf;/Users/franzilow/Zotero/storage/IN7HF2YE/1508.html}
}

@article{steiner_program_1952,
  title = {Program {{Patterns}} and {{Preferences}}, and the {{Workability}} of {{Competition}} in {{Radio Broadcasting}}},
  author = {Steiner, Peter O.},
  date = {1952},
  journaltitle = {The Quarterly Journal of Economics},
  volume = {66},
  number = {2},
  pages = {194--223},
  issn = {0033-5533},
  doi = {10.2307/1882942},
  url = {http://www.jstor.org/stable/1882942},
  abstract = {I. Criteria for the appraisal of workability, 195.--II. The one period model, 197.--III. The model over time, 207.--IV. Relevance of the model to the market structure of the industry, 217.--V. Some suggestions for further analysis, 222.}
}

@software{stewart_bstewart_2021,
  title = {Bstewart/Stm},
  author = {Stewart, Brandon},
  date = {2021-09-29T09:48:24Z},
  origdate = {2014-02-04T21:15:10Z},
  url = {https://github.com/bstewart/stm},
  urldate = {2021-10-10},
  abstract = {An R Package for the Structural Topic Model}
}

@incollection{steyvers_probabilistic_2006,
  title = {Probabilistic {{Topic Models}}},
  booktitle = {Latent {{Semantic Analysis}}: {{A Road}} to {{Meaning}}.},
  author = {Steyvers, Mark and Griffiths, Thomas L.},
  editor = {Landauer, L. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
  date = {2006},
  publisher = {{Laurence Erlbaum}}
}

@article{stromback_four_2008,
  title = {Four {{Phases}} of {{Mediatization}}: {{An Analysis}} of the {{Mediatization}} of {{Politics}}},
  shorttitle = {Four {{Phases}} of {{Mediatization}}},
  author = {Strömbäck, Jesper},
  date = {2008-07-01},
  journaltitle = {The International Journal of Press/Politics},
  shortjournal = {The International Journal of Press/Politics},
  volume = {13},
  number = {3},
  pages = {228--246},
  issn = {1940-1612},
  doi = {10.1177/1940161208319097},
  url = {https://doi.org/10.1177/1940161208319097},
  urldate = {2018-10-20},
  abstract = {Two concepts that have been used to describe the changes with regards to media and politics during the last fifty years are the concepts of mediation and mediatization . However, both these concepts are used more often than they are properly defined. Moreover, there is a lack of analysis of the process of mediatization, although the concept as such denotes a process.Thus the purpose of this article is to analyze the concepts of mediated and mediatized politics from a process-oriented perspective. The article argues that mediatization is a multidimensional and inherently process-oriented concept and that it is possible to make a distinction between four phases of mediatization. Each of these phases is analyzed.The conclusion is that as politics becomes increasingly mediatized, the important question no longer is related to the independence of the media from politics and society. The important question becomes the independence of politics and society from the media.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/IHSBRJN6/Strömbäck - 2008 - Four Phases of Mediatization An Analysis of the M.pdf}
}

@article{stromberg_radio_2004,
  title = {Radio's {{Impact}} on {{Public Spending}}},
  author = {Strömberg, David},
  date = {2004-02-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {Q J Econ},
  volume = {119},
  number = {1},
  pages = {189--221},
  issn = {0033-5533},
  doi = {10.1162/003355304772839560},
  url = {https://academic.oup.com/qje/article/119/1/189/1876059},
  urldate = {2019-01-11},
  abstract = {Abstract.  If informed voters receive favorable policies, then the invention of a new mass medium may affect government policies since it affects who is informe},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/NUNIULNN/Strömberg - 2004 - Radio's Impact on Public Spending.pdf;/Users/franzilow/Zotero/storage/UAJ5MGAP/1876059.html}
}

@article{suen_selfperpetuation_2004,
  title = {The {{Self-Perpetuation}} of {{Biased Beliefs}}},
  author = {Suen, Wing},
  date = {2004},
  journaltitle = {Economic Journal},
  volume = {114},
  number = {495},
  pages = {377--396},
  url = {https://ideas.repec.org/a/ecj/econjl/v114y2004i495p377-396.html},
  urldate = {2019-01-19},
  abstract = {To overcome strong prior beliefs, strong evidence to the contrary is needed. If a person is predisposed to choosing a certain action, the advice from an advisor who sets a low threshold for recommending the alternative action is not of much use. The preference for like-minded advisors who supply coarse information implies that the advice a person receives is likely to reinforce his existing priors. This effect can lead to polarisation of opinion and the emergence of self-serving beliefs. The learning process is prolonged and the induced short run bias can become perpetual if information is costly. Copyright 2004 Royal Economic Society.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/BSUZCHV8/v114y2004i495p377-396.html}
}

@inproceedings{taddy_estimation_2012,
  title = {On Estimation and Selection for Topic Models},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Taddy, Matt},
  date = {2012}
}

@article{takens_media_2013,
  title = {Media Logic in Election Campaign Coverage},
  author = {Takens, Janet and Atteveldt, Wouter and family=Hoof, given=Anita, prefix=van, useprefix=true and Kleinnijenhuis, Jan},
  date = {2013-06-01},
  journaltitle = {European Journal of Communication},
  shortjournal = {European Journal of Communication},
  volume = {28},
  pages = {277--293},
  doi = {10.1177/0267323113478522},
  abstract = {The media logic thesis holds that the content of political news is the product of news values and format requirements that media make use of to attract news consumers. This study tests whether three content characteristics – personalized, contest and negative coverage – manifest a single media logic by analysing whether they co-vary over time. It also tests the implicit assumption underlying the media logic thesis that media adhere to a single media logic as one institution. A semantic network analysis measured the degree to which television and newspaper coverage of five Dutch national election campaigns (1998–2010) contained the three content characteristics. The study shows that personalized, contest and negative coverage form three indicators of a single logic that is shared by different media. Since the turn of the century, Dutch political news has simultaneously become decreasingly personalized, less focused on the contest and less negative.}
}

@article{takens_old_2010,
  title = {Old Ties from a New(s) Perspective: {{Diversity}} in the {{Dutch}} Press Coverage of the 2006 General Election Campaign},
  shorttitle = {Old Ties from a New(s) Perspective},
  author = {Takens, Janet and Ruigrok, Nel and family=Hoof, given=Anita, prefix=van, useprefix=true and Scholten, Otto},
  date = {2010-11-01},
  journaltitle = {Appetite},
  shortjournal = {Appetite},
  volume = {35},
  pages = {417--438},
  doi = {10.1515/comm.2010.022},
  abstract = {This study examines the extent to which the highly diverse and volatile Dutch electorate received a diverse offer of political newspaper coverage during the 2006 general election campaign. We measured the level of diversity of five subscription based national newspapers with a partisan history and two free dailies. Two forms of diversity were examined: party diversity (i. e., the distribution of attention to political parties) and issue diversity (i. e., the distribution of attention to issues). The diversity of party coverage in the free dailies was greater than the diversity across all newspapers. Whereas free dailies paid a relatively large amount of attention to new and opposition parties, traditional newspapers paid a relatively large amount of attention to the parties with whom they were aligned during the period of “pillarization”. Conversely, we only found small differences in the distribution of attention to issues. The diversity of issue coverage across newspapers was larger than the diversity within newspapers.}
}

@article{teh_hierarchical_2006,
  title = {Hierarchical {{Dirichlet Processes}}},
  author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
  date = {2006-12-01},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {101},
  number = {476},
  pages = {1566--1581},
  issn = {0162-1459},
  doi = {10.1198/016214506000000302},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
  abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
  file = {/Users/franzilow/Zotero/storage/67GVBDZ8/016214506000000302.html}
}

@article{tetlock_giving_2007,
  title = {Giving {{Content}} to {{Investor Sentiment}}: {{The Role}} of {{Media}} in the {{Stock Market}}},
  shorttitle = {Giving {{Content}} to {{Investor Sentiment}}},
  author = {Tetlock, Paul C.},
  date = {2007-06-01},
  journaltitle = {The Journal of Finance},
  volume = {62},
  number = {3},
  pages = {1139--1168},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.2007.01232.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
  abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/R4FBIJ83/abstract.html}
}

@article{tetlock_giving_2007a,
  title = {Giving {{Content}} to {{Investor Sentiment}}: {{The Role}} of {{Media}} in the {{Stock Market}}},
  shorttitle = {Giving {{Content}} to {{Investor Sentiment}}},
  author = {Tetlock, Paul C.},
  date = {2007-06-01},
  journaltitle = {The Journal of Finance},
  volume = {62},
  number = {3},
  pages = {1139--1168},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.2007.01232.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
  urldate = {2018-03-07},
  abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/RHRD6AQT/Tetlock - 2007 - Giving Content to Investor Sentiment The Role of .pdf}
}

@article{tetlock_more_2008,
  title = {More {{Than Words}}: {{Quantifying Language}} to {{Measure Firms}}' {{Fundamentals}}},
  shorttitle = {More {{Than Words}}},
  author = {Tetlock, Paul C. and Saar-Tsechansky, Maytal and Macskassy, Sofus},
  date = {2008-06-01},
  journaltitle = {The Journal of Finance},
  volume = {63},
  number = {3},
  pages = {1437--1467},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.2008.01362.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2008.01362.x/abstract},
  urldate = {2018-03-07},
  abstract = {We examine whether a simple quantitative measure of language can be used to predict individual firms' accounting earnings and stock returns. Our three main findings are: (1) the fraction of negative words in firm-specific news stories forecasts low firm earnings; (2) firms' stock prices briefly underreact to the information embedded in negative words; and (3) the earnings and return predictability from negative words is largest for the stories that focus on fundamentals. Together these findings suggest that linguistic media content captures otherwise hard-to-quantify aspects of firms' fundamentals, which investors quickly incorporate into stock prices.},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/NTZB3TUK/Tetlock et al. - 2008 - More Than Words Quantifying Language to Measure F.pdf}
}

@article{thistlethwaite_regressiondiscontinuity_1960,
  title = {Regression-Discontinuity Analysis: {{An}} Alternative to the Ex Post Facto Experiment},
  shorttitle = {Regression-Discontinuity Analysis},
  author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
  date = {1960},
  journaltitle = {Journal of Educational Psychology},
  volume = {51},
  number = {6},
  pages = {309--317},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-2176(Electronic),0022-0663(Print)},
  doi = {10.1037/h0044319},
  abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners—5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Academic Achievement,Causal Analysis,Elementary School Students,Experimentation,Occupational Choice,Statistical Regression,Student Attitudes,Testing},
  file = {/Users/franzilow/Zotero/storage/RCEPWPV4/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/franzilow/Zotero/storage/XYX5CZFL/1962-00061-001.html}
}

@inproceedings{tumasjan_predicting_2010,
  title = {Predicting {{Elections}} with {{Twitter}}: {{What}} 140 {{Characters Reveal}} about {{Political Sentiment}}},
  shorttitle = {Predicting {{Elections}} with {{Twitter}}},
  booktitle = {Proceedings of the {{Fourth International Conference}} on {{Weblogs}} and {{Social Media}}},
  author = {Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp and Welpe, Isabell},
  date = {2010},
  location = {{Washington}},
  url = {https://www.researchgate.net/publication/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentiment},
  urldate = {2018-03-17},
  abstract = {Twitter is a microblogging website where users read and write millions of short messages on a variety of topics every day. This study uses the context of the German federal election to investigate whether Twitter is used as a forum for political deliberation and whether online messages on Twitter validly mirror offline political sentiment. Using LIWC text analysis software, we conducted a contentanalysis of over 100,000 messages containing a reference to either a political party or a politician. Our results show that Twitter is indeed used extensively for political deliberation. We find that the mere number of messages mentioning a party reflects the election result. Moreover, joint mentions of two parties are in line with real world political ties and coalitions. An analysis of the tweets' political sentiment demonstrates close correspondence to the parties' and politicians' political positions indicating that the content of Twitter messages plausibly reflects the offline political landscape. We discuss the use of microblogging message content as a valid indicator of political sentiment and derive suggestions for further research.},
  eventtitle = {{{INTERNATIONAL AAAI CONFERENCE ON WEBLOGS AND SOCIAL MEDIA}}},
  langid = {english},
  file = {/Users/franzilow/Zotero/storage/JHIPIJAW/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentimen.html}
}

@article{vanderklaauw_estimating_2002,
  title = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}: {{A Regression-Discontinuity Approach}}},
  shorttitle = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}},
  author = {family=Klaauw, given=Wilbert, prefix=van der, useprefix=true},
  date = {2002},
  journaltitle = {International Economic Review},
  volume = {43},
  number = {4},
  pages = {1249--1287},
  publisher = {{[Economics Department of the University of Pennsylvania, Wiley, Institute of Social and Economic Research, Osaka University]}},
  issn = {0020-6598},
  url = {https://www.jstor.org/stable/826967},
  urldate = {2021-06-20},
  abstract = {An important problem faced by colleges and universities, that of evaluating the effect of their financial aid offers on student enrollment decisions, is complicated by the likely endogeneity of the aid offer variable in a student enrollment equation. This article shows how discontinuities in an East Coast college's aid assignment rule can be exploited to obtain credible estimates of the aid effect without having to rely on arbitrary exclusion restrictions and functional form assumptions. Semiparametric estimates based on a regression-discontinuity (RD) approach affirm the importance of financial aid as an effective instrument in competing with other colleges for students.}
}

@inproceedings{wallach_evaluation_2009,
  title = {Evaluation {{Methods}} for {{Topic Models}}},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
  date = {2009},
  series = {{{ICML}} '09},
  pages = {1105--1112},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553515},
  url = {http://doi.acm.org/10.1145/1553374.1553515},
  abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
  isbn = {978-1-60558-516-1}
}

@incollection{wallach_rethinking_2009,
  title = {Rethinking {{LDA}}: {{Why Priors Matter}}},
  shorttitle = {Rethinking {{LDA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Wallach, Hanna M. and Mimno, David M. and McCallum, Andrew},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  date = {2009},
  pages = {1973--1981},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf},
  file = {/Users/franzilow/Zotero/storage/TPHKVI25/Wallach et al. - 2009 - Rethinking LDA Why Priors Matter.pdf;/Users/franzilow/Zotero/storage/6FVP9RXD/3854-rethinking-lda-why-priors-matter.html}
}

@inproceedings{wang_mining_2009,
  title = {Mining {{Common Topics}} from {{Multiple Asynchronous Text Streams}}},
  booktitle = {Proceedings of the {{Second ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Wang, Xiang and Zhang, Kai and Jin, Xiaoming and Shen, Dou},
  date = {2009},
  series = {{{WSDM}} '09},
  pages = {192--201},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1498759.1498826},
  url = {http://doi.acm.org/10.1145/1498759.1498826},
  urldate = {2018-01-23},
  abstract = {Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.},
  isbn = {978-1-60558-390-7},
  keywords = {asynchronous streams,temporal text mining,topic model}
}

@inproceedings{wei_ldabased_2006,
  title = {{{LDA-based Document Models}} for {{Ad-hoc Retrieval}}},
  booktitle = {Proceedings of the 29th {{Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wei, Xing and Croft, W. Bruce},
  date = {2006},
  series = {{{SIGIR}} '06},
  pages = {178--185},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1148170.1148204},
  url = {http://doi.acm.org/10.1145/1148170.1148204},
  abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
  isbn = {978-1-59593-369-0},
  keywords = {document model,Information retrieval,language model,latent dirichlet allocation (LDA),topic model}
}

@book{wiedmann_text_2016,
  title = {Text {{Mining}} for {{Qualitative Data Analysis}} in the {{Social Sciences}}},
  author = {Wiedmann, Gregor},
  date = {2016},
  edition = {1},
  publisher = {{VS Verlag für Sozialwissenschaften}},
  location = {{Wiesbaden}},
  url = {//www.springer.com/de/book/9783658153083},
  urldate = {2017-11-26},
  file = {/Users/franzilow/Zotero/storage/7P4K4CSX/9783658153083.html}
}


